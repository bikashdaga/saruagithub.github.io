<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>WangXue</title>
  
  <subtitle>快乐学习，慢慢赚钱</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://yoursite.com/"/>
  <updated>2019-12-23T14:13:45.711Z</updated>
  <id>http://yoursite.com/</id>
  
  <author>
    <name>WangXue</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Stochastic Gradient Descent</title>
    <link href="http://yoursite.com/2019/12/23/20181015StochasticGradientDescent/"/>
    <id>http://yoursite.com/2019/12/23/20181015StochasticGradientDescent/</id>
    <published>2019-12-23T02:20:15.000Z</published>
    <updated>2019-12-23T14:13:45.711Z</updated>
    
    <content type="html"><![CDATA[<h3 id="1-概述"><a href="#1-概述" class="headerlink" title="1 概述"></a>1 概述</h3><h4 id="1-1-梯度下降"><a href="#1-1-梯度下降" class="headerlink" title="1.1 梯度下降"></a>1.1 梯度下降</h4><p>梯度下降是经典的局部优化算法。在2000年L Bottou使得随机梯度下降再次被提出。</p><p>对于数据$\left\{\left(X_{j}, Y_{j}\right)\right\}_{j=1}^{M}$ 需要求解：</p><script type="math/tex; mode=display">\min _{\theta \in \mathbb{R}^{n}} J(\theta), \quad J(\theta)=\frac{1}{M} \sum_{j=1}^{M} L\left(\theta ; X_{j}, Y_{j}\right)</script><p>梯度下降迭代格式：</p><script type="math/tex; mode=display">\theta_{i+1}=\theta_{i}-\alpha_{i} \nabla J\left(\theta_{i}\right), \quad \alpha_{i} \in \mathbb{R}^{+}</script><p>直接针对损失函数的梯度下降存在的问题是容易陷入局部极小，计算量大（每一次都要计算$\nabla_{\theta} L\left(\theta_{i} ; X_{j}, Y_{j}\right)$）, 鞍点终止问题（鞍点梯度为0）。</p><h4 id="1-2-随机梯度下降"><a href="#1-2-随机梯度下降" class="headerlink" title="1.2 随机梯度下降"></a>1.2 随机梯度下降</h4><p>因此提出随机梯度下降。每次仅仅随机取一个数据$\left(X_{R_{i}}, Y_{R_{i}}\right)$来近似均值的损失$\frac{1}{M} \sum_{j=1}^{M} \nabla_{\theta} L\left(\theta_{i} ; X_{j}, Y_{j}\right)$。</p><script type="math/tex; mode=display">\theta_{i+1}=\theta_{i}-\alpha_{i} \nabla_{\theta} L\left(\theta_{i} ; X_{R_{i}}, Y_{R_{i}}\right)</script><h4 id="1-3-三种梯度下降"><a href="#1-3-三种梯度下降" class="headerlink" title="1.3 三种梯度下降"></a>1.3 三种梯度下降</h4><p>梯度下降：全部数据迭代计算梯度。</p><script type="math/tex; mode=display">\theta_{i+1}=\theta_{i}-\alpha_{i} \frac{1}{M} \sum_{j=1}^{M} \nabla_{\theta} L\left(\theta_{i} ; X_{j}, Y_{j}\right)</script><p>随机梯度：随机取一个数据来更新梯度。</p><script type="math/tex; mode=display">\theta_{i+1}=\theta_{i}-\alpha_{i} \nabla_{\theta} L\left(\theta_{i} ; X_{R_{i}}, Y_{R_{i}}\right)</script><p>小批量梯度：随机取$m(\in[50,300])$ 个数据来计算梯度。</p><script type="math/tex; mode=display">\theta_{i+1}=\theta_{i}-\alpha_{i} \frac{1}{m} \sum_{j=1}^{m} \nabla_{\theta} L\left(\theta_{i} ; X_{R_{i, j}}, Y_{R_{i, j}}\right)</script><p>下批量梯度下降的好处。加噪，避免梯度法终止于鞍点，存在一定概率跳出局部极小，小批量计算量可接受。</p><p>如果$J_M(\theta)$满足强凸条件，对于批量梯度法，线性收敛。对于随机梯度下降法，次线性收敛。</p><h4 id="1-4-Github代码"><a href="#1-4-Github代码" class="headerlink" title="1.4 Github代码"></a>1.4 Github代码</h4><p><a href="https://github.com/saruagithub/AIcourse_gradientDescent" target="_blank" rel="noopener">https://github.com/saruagithub/AIcourse_gradientDescent</a></p><h3 id="2-SGD技巧"><a href="#2-SGD技巧" class="headerlink" title="2 SGD技巧"></a>2 SGD技巧</h3><p>1，SGD缺点：梯度方向不一定好，固定的学习率太小收敛慢太大则阻碍收敛，如何快速穿过山谷（狭窄山谷的震荡）平原呢。</p><script type="math/tex; mode=display">v_i = \alpha \nabla_{\theta} J\left(\theta_{i}\right)</script><script type="math/tex; mode=display">\theta_{i+1}=\theta_{i}-v_{i}</script><p>2，动量法：梯度的加权平均，递归的添加方向的历史信息（即$v_{i-1}$）。但转弯会慢。</p><script type="math/tex; mode=display">v_{i}=\gamma v_{i-1}+\alpha \nabla_{\theta} J\left(\theta_{i}\right)</script><p>其中$\gamma$ 是阻力因子。</p><script type="math/tex; mode=display">\theta_{i+1}=\theta_{i}-v_{i}</script><p>3，Nesterov：加速梯度法，更早的注意到梯度的变化。在动量法梯度更新前减去动量项。</p><script type="math/tex; mode=display">v_{i}=\gamma v_{i-1}+\alpha \nabla_{\theta} J\left(\theta_{i}-\gamma v_{i-1}\right)</script><script type="math/tex; mode=display">\theta_{i+1}=\theta_{i}-v_{i}</script><p>就是使用上一步的$v_{i-1}$先走一步再计算合并梯度。这里的$- \gamma v_{i-1}$就是下图B-C这段。</p><p>优点：前瞻性，在原方向虚拟走了一步后的梯度。收敛速度明显加快。波动也小了很多。</p><p><img src="/images/20181015Nesterov.jpg" alt="20181015Nesterov"></p><p>4，Adagrad：自适应梯度，弱化频繁变化的参数。$G_i$指的是历史与当前梯度的平方的累加。</p><script type="math/tex; mode=display">\theta_{i+1}=\theta_{i}-\frac{\alpha}{\sqrt{G_{i}+\epsilon}} \nabla_{\theta} J\left(\theta_{i}\right)</script><script type="math/tex; mode=display">G_{i}=G_{i-1} + (\nabla_{\theta} J\left(\theta_{i}\right))^{2}</script><p>$\epsilon$ 平滑项，避免除数为0。</p><p>5，RMSProp，对AdaGrad的一种改进，使用加权平均于梯度平方项。当前梯度平方项加上上一时刻的平均值。</p><script type="math/tex; mode=display">G_{i}=\gamma G_{i-1}+(1-\gamma)(\nabla_{\theta} J\left(\theta_{i}\right))^{2}</script><script type="math/tex; mode=display">\theta_{i+1}=\theta_{i}-\frac{\alpha}{\sqrt{G_{i}+\epsilon}} \nabla_{\theta} J\left(\theta_{i}\right)</script><p>另一个改进是定义指数衰减均值，AdaDelta2使用Delta平方的exponential moving average替代learning rate。</p><script type="math/tex; mode=display">\theta_{i+1} = \theta_i -\frac{\sqrt{D_{i-1}+\epsilon}}{\sqrt{G_{i}+\epsilon}} \nabla_{\theta} J(\theta_{i})</script><script type="math/tex; mode=display">D_{i}=\gamma D_{i-1}+(1-\gamma)\left[\Delta \theta_{t}\right]^{2}</script><script type="math/tex; mode=display">G_{i}=\gamma G_{i-1}+(1-\gamma)(\nabla_{\theta} J\left(\theta_{i}\right))^{2}</script><script type="math/tex; mode=display">\Delta \theta_{t}=\theta_{t}-\theta_{t-1}</script><p>6，Adam:Adam是对Momentum和RMPprop的一个结合。像 Adadelta 和 RMSprop 一样存储了过去梯度的平方 vt 的指数衰减平均值 ，也像 momentum 一样保持了过去梯度 mt 的指数衰减平均值。</p><p>首先令：</p><script type="math/tex; mode=display">v_{i}=\gamma_{1} v_{i-1}+\left(1-\gamma_{1}\right) \nabla_{\theta} J\left(\theta_{i}\right)</script><script type="math/tex; mode=display">u_{i}=\gamma_{2} u_{i-1}+\left(1-\gamma_{2}\right) (\nabla_{\theta} J\left(\theta_{i}\right))^{2}</script><p>则：</p><script type="math/tex; mode=display">\hat{v}_{i}=\frac{v_{i}}{1-\gamma_{1}^{i}}, \quad \hat{u}_{i}=\frac{u_{i}}{1-\gamma_{2}^{i}}</script><p>最终得：</p><script type="math/tex; mode=display">\theta_{i+1} = \theta_i - \frac{\alpha}{\sqrt{\hat{u}_{i}+\epsilon}} \hat{v}_{i}</script><p>梯度部分像Momentum里一样使用V即梯度的exponential moving average来替代当前梯度来更新权重。学习率部分像RMSprop里一样用学习率除以S(即梯度的exponential moving average)来进行学习。V和S都初始化为0。一般$\alpha=0.001, \quad \gamma_{1}=0.9, \quad \gamma_{2}=0.999, \quad \epsilon=10^{-8}$</p><p>7, 推荐技巧：</p><script type="math/tex; mode=display">v_{i}=\gamma v_{i-1}+(1-\gamma) \nabla_{\theta} J\left(\theta_{i}-\gamma v_{i-1}\right)</script><script type="math/tex; mode=display">u_{i}=\gamma u_{i-1}+(1-\gamma)\left(\nabla_{\theta} J\left(\theta_{i}-\gamma v_{i-1}\right)\right)^{2}</script><script type="math/tex; mode=display">w_{i}=\gamma w_{i-1}+(1-\gamma) \Delta \theta_{i}^{2}</script><p>可得：</p><script type="math/tex; mode=display">\theta_{i+1}=\theta_{i} -\frac{\sqrt{w_{i-1}+\epsilon}}{\sqrt{u_{i}+\epsilon}} v_{i}</script><p>所有技巧的目的都是为了根据历史梯度和当前梯度来更新梯度。学习率迭代则是为了能适应梯度，梯度太大则更新小，将学习率learning rate除以当前的梯度，就能得到一个“适应”好的学习率的值。</p><p>数学回顾：一元函数的导数与泰勒级数</p><p>函数f(x)在x0上的导数定义为：</p><script type="math/tex; mode=display">f^{\prime}\left(x_{0}\right)=\lim _{x \rightarrow x_{0}} \frac{f(x)-f\left(x_{0}\right)}{x-x_{0}}</script><p>f(x)在x0附近的Taylor级数是：</p><script type="math/tex; mode=display">f(x)=f\left(x_{0}\right)+f^{\prime}\left(x_{0}\right)\left(x-x_{0}\right)+\frac{f^{\prime \prime}\left(x_{0}\right)}{2}\left(x-x_{0}\right)^{2}+O\left(\left|x-x_{0}\right|^{3}\right)</script><h3 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h3><p>1，智能技术基础课PPT &amp; 印象笔记，智能技术基础课2，3</p><p>2，<a href="https://blog.csdn.net/tsyccnh/article/details/76673073" target="_blank" rel="noopener">https://blog.csdn.net/tsyccnh/article/details/76673073</a></p><p>3, <a href="https://www.zhihu.com/question/305638940/answer/770984541" target="_blank" rel="noopener">https://www.zhihu.com/question/305638940/answer/770984541</a> 梯度下降法</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h3 id=&quot;1-概述&quot;&gt;&lt;a href=&quot;#1-概述&quot; class=&quot;headerlink&quot; title=&quot;1 概述&quot;&gt;&lt;/a&gt;1 概述&lt;/h3&gt;&lt;h4 id=&quot;1-1-梯度下降&quot;&gt;&lt;a href=&quot;#1-1-梯度下降&quot; class=&quot;headerlink&quot; title=&quot;1
      
    
    </summary>
    
    
      <category term="深度学习" scheme="http://yoursite.com/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="机器学习" scheme="http://yoursite.com/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="深度学习" scheme="http://yoursite.com/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>leecode168周赛</title>
    <link href="http://yoursite.com/2019/12/22/20191222leecode168%E5%91%A8%E8%B5%9B/"/>
    <id>http://yoursite.com/2019/12/22/20191222leecode168%E5%91%A8%E8%B5%9B/</id>
    <published>2019-12-22T07:06:13.000Z</published>
    <updated>2019-12-22T10:08:55.195Z</updated>
    
    <content type="html"><![CDATA[<h3 id="leecode5291统计位数为偶数的数字"><a href="#leecode5291统计位数为偶数的数字" class="headerlink" title="leecode5291统计位数为偶数的数字"></a>leecode5291统计位数为偶数的数字</h3><p>给你一个整数数组 <code>nums</code>，请你返回其中位数为 <strong>偶数</strong> 的数字的个数。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">输入：nums = [12,345,2,6,7896]</span><br><span class="line">输出：2</span><br><span class="line">解释：</span><br><span class="line">12 是 2 位数字（位数为偶数） </span><br><span class="line">345 是 3 位数字（位数为奇数）  </span><br><span class="line">2 是 1 位数字（位数为奇数） </span><br><span class="line">6 是 1 位数字 位数为奇数） </span><br><span class="line">7896 是 4 位数字（位数为偶数）  </span><br><span class="line">因此只有 12 和 7896 是位数为偶数的数字</span><br><span class="line"></span><br><span class="line">输入：nums = [555,901,482,1771]</span><br><span class="line">输出：1 </span><br><span class="line">解释： </span><br><span class="line">只有 1771 是位数为偶数的数字。</span><br><span class="line"></span><br><span class="line">1 &lt;= nums.length &lt;= 500</span><br><span class="line">1 &lt;= nums[i] &lt;= 10^5</span><br></pre></td></tr></table></figure><p>思路1：c++，位数是除以10，而判断是否偶数是对2取余判断是否为0。</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;stdio.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;vector&gt;</span></span></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> <span class="built_in">std</span>;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">findNumbers</span><span class="params">(<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;&amp; nums)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">int</span> res=<span class="number">0</span>;</span><br><span class="line">    <span class="keyword">for</span>(<span class="keyword">auto</span> num: nums)&#123;</span><br><span class="line">        <span class="keyword">int</span> weishu=<span class="number">1</span>;</span><br><span class="line">        <span class="keyword">while</span>(num / <span class="number">10</span> &gt; <span class="number">0</span>)&#123;</span><br><span class="line">            weishu ++;</span><br><span class="line">            num /= <span class="number">10</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="built_in">cout</span>&lt;&lt;weishu&lt;&lt;<span class="built_in">endl</span>;</span><br><span class="line">        <span class="keyword">if</span>(weishu % <span class="number">2</span> == <span class="number">0</span>)&#123;</span><br><span class="line">            res ++;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> res;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span></span>&#123;</span><br><span class="line">    <span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt; test1 = &#123;<span class="number">12</span>,<span class="number">345</span>,<span class="number">2</span>,<span class="number">6</span>,<span class="number">7896</span>&#125;;</span><br><span class="line">    <span class="keyword">int</span> res = findNumbers(test1);</span><br><span class="line">    <span class="built_in">cout</span>&lt;&lt;res&lt;&lt;<span class="built_in">endl</span>;</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>思路2：Python，遍历nums里的数字。将数字转换为string。判断string的长度对2取余是否为0，是0则取1，否则取0（表示位数不是偶数）。再将是偶数的数字求和 sum。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">findNumbers</span><span class="params">(nums: List[int])</span> -&gt; int:</span></span><br><span class="line">        <span class="keyword">return</span> sum(<span class="number">1</span> <span class="keyword">if</span> len(str(x)) % <span class="number">2</span> == <span class="number">0</span> <span class="keyword">else</span> <span class="number">0</span> <span class="keyword">for</span> x <span class="keyword">in</span> nums)</span><br></pre></td></tr></table></figure><h3 id="leecode5292划分数组为连续数字的集合"><a href="#leecode5292划分数组为连续数字的集合" class="headerlink" title="leecode5292划分数组为连续数字的集合"></a>leecode5292划分数组为连续数字的集合</h3><p>给你一个整数数组 nums 和一个正整数 k，请你判断是否可以把这个数组划分成一些由 k 个连续数字组成的集合。<br>如果可以，请返回 True；否则，返回 False。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">实例1</span><br><span class="line">输入：nums = [1,2,3,3,4,4,5,6], k = 4</span><br><span class="line">输出：true</span><br><span class="line">解释：数组可以分成 [1,2,3,4] 和 [3,4,5,6]。</span><br><span class="line"></span><br><span class="line">示例2</span><br><span class="line">输入：nums = [3,2,1,2,3,4,3,4,5,9,10,11], k = 3</span><br><span class="line">输出：true</span><br><span class="line">解释：数组可以分成 [1,2,3] , [2,3,4] , [3,4,5] 和 [9,10,11]。</span><br><span class="line"></span><br><span class="line">示例3 </span><br><span class="line">输入：nums = [3,3,2,2,1,1], k = 3</span><br><span class="line">输出：true</span><br><span class="line"></span><br><span class="line">示例4</span><br><span class="line">输入：nums = [1,2,3,4], k = 3</span><br><span class="line">输出：false</span><br><span class="line">解释：数组不能分成几个大小为 3 的子数组。</span><br><span class="line"></span><br><span class="line">1 &lt;= nums.length &lt;= 10^5</span><br><span class="line">1 &lt;= nums[i] &lt;= 10^9</span><br><span class="line">1 &lt;= k &lt;= nums.length</span><br></pre></td></tr></table></figure><p>思路1：简单基本思路，将nums里的最小取出来，然后每次取[min,min+k]的值，不断从原nums里去除掉。如果可以这样去空原nums则返回true，否则只要有值不在nums里，则返回false。（但这个方法的时间复杂度太高$O(kn^2)$）</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -*- coding:utf-8 -*-</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">isPossibleDivide</span><span class="params">(nums, k)</span>:</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">:type nums: List[int]</span></span><br><span class="line"><span class="string">:type k: int</span></span><br><span class="line"><span class="string">:rtype: bool</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="keyword">if</span> (len(nums) % k != <span class="number">0</span>):</span><br><span class="line"><span class="keyword">return</span> <span class="literal">False</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">while</span> (len(nums) != <span class="number">0</span>):</span><br><span class="line"><span class="comment"># each list</span></span><br><span class="line"><span class="keyword">for</span> x <span class="keyword">in</span> range(min(nums), min(nums)+k):</span><br><span class="line"><span class="keyword">if</span> x <span class="keyword">in</span> nums:</span><br><span class="line">nums.remove(x)</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line"><span class="keyword">return</span> <span class="literal">False</span></span><br><span class="line"><span class="keyword">return</span> <span class="literal">True</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">nums = [<span class="number">3</span>,<span class="number">2</span>,<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>,<span class="number">9</span>,<span class="number">10</span>,<span class="number">11</span>]</span><br><span class="line">k = <span class="number">3</span></span><br><span class="line">res = isPossibleDivide(nums,k)</span><br><span class="line">print(res)</span><br></pre></td></tr></table></figure><p>进一步，用hash优化查找x in nums。(c++中的map是平衡二叉树)，排序时间复杂度$O(nlogn)$，</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;stdio.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;vector&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;map&gt;</span></span></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> <span class="built_in">std</span>;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">bool</span> <span class="title">isPossibleDivide</span><span class="params">(<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;&amp; nums, <span class="keyword">int</span> k)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">if</span> (nums.<span class="built_in">size</span>() % k != <span class="number">0</span>) &#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    sort(nums.<span class="built_in">begin</span>(), nums.<span class="built_in">end</span>());</span><br><span class="line">    <span class="built_in">map</span>&lt;<span class="keyword">int</span>,<span class="keyword">int</span>&gt; hash;</span><br><span class="line">    <span class="keyword">for</span>(<span class="keyword">auto</span> num:nums) hash[num]++;</span><br><span class="line">    <span class="keyword">int</span> groups = nums.<span class="built_in">size</span>() / k;</span><br><span class="line">    <span class="comment">//group nums</span></span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> i=<span class="number">0</span>; i&lt;groups; i++) &#123;</span><br><span class="line">        <span class="keyword">int</span> min_index = <span class="number">0</span>;</span><br><span class="line">        <span class="keyword">while</span> (hash[nums[min_index]] == <span class="number">0</span>) &#123;</span><br><span class="line">            min_index++;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="comment">//if min~min+k is not in nums, false</span></span><br><span class="line">        <span class="keyword">int</span> start = nums[min_index];</span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> j=start; j&lt;start+k; j++) &#123;</span><br><span class="line">            <span class="keyword">if</span> (hash[j] == <span class="number">0</span>) &#123;</span><br><span class="line">                <span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="keyword">else</span> hash[j]--;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> <span class="literal">true</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span></span>&#123;</span><br><span class="line">    <span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt; test2 = &#123;<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">4</span>,<span class="number">5</span>,<span class="number">9</span>,<span class="number">10</span>,<span class="number">11</span>&#125;;</span><br><span class="line">    <span class="keyword">int</span> res2 = isPossibleDivide(test2,<span class="number">3</span>);</span><br><span class="line">    <span class="built_in">cout</span>&lt;&lt;res2&lt;&lt;<span class="built_in">endl</span>;</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>思路3。新学习了multiset。避免了while这一段找min_index（见上），直接在multiset里查找并去掉，时间要短一点点，但空间用的要更多（因为multiset允许存储重复元素）</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">bool</span> <span class="title">isPossibleDivide2</span><span class="params">(<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;&amp; nums, <span class="keyword">int</span> k)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">if</span> (nums.<span class="built_in">size</span>() % k != <span class="number">0</span>) &#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="built_in">multiset</span>&lt;<span class="keyword">int</span>&gt; s;</span><br><span class="line">    <span class="keyword">for</span>(<span class="keyword">auto</span> num:nums) s.insert(num);</span><br><span class="line">  <span class="comment">//multiset&lt;int&gt; s(a.begin(), a.end());</span></span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> i=<span class="number">0</span>; i&lt;nums.<span class="built_in">size</span>() / k; i++) &#123;</span><br><span class="line">        <span class="keyword">int</span> <span class="built_in">min</span> = *s.<span class="built_in">begin</span>();</span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> j=<span class="built_in">min</span>; j&lt;<span class="built_in">min</span>+k; j++) &#123;</span><br><span class="line">            <span class="keyword">if</span> (s.<span class="built_in">find</span>(j) == s.<span class="built_in">end</span>()) &#123;</span><br><span class="line">                <span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line">            &#125;</span><br><span class="line">            s.erase(s.<span class="built_in">find</span>(j));</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> <span class="literal">true</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h3><p>1, <a href="https://leetcode-cn.com/problems/find-numbers-with-even-number-of-digits" target="_blank" rel="noopener">https://leetcode-cn.com/problems/find-numbers-with-even-number-of-digits</a></p><p>2, <a href="https://leetcode-cn.com/problems/divide-array-in-sets-of-k-consecutive-numbers" target="_blank" rel="noopener">https://leetcode-cn.com/problems/divide-array-in-sets-of-k-consecutive-numbers</a></p><p>3，<a href="https://leetcode-cn.com/contest/weekly-contest-168/" target="_blank" rel="noopener">https://leetcode-cn.com/contest/weekly-contest-168/</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h3 id=&quot;leecode5291统计位数为偶数的数字&quot;&gt;&lt;a href=&quot;#leecode5291统计位数为偶数的数字&quot; class=&quot;headerlink&quot; title=&quot;leecode5291统计位数为偶数的数字&quot;&gt;&lt;/a&gt;leecode5291统计位数为偶数的数字&lt;/
      
    
    </summary>
    
    
      <category term="算法" scheme="http://yoursite.com/categories/%E7%AE%97%E6%B3%95/"/>
    
    
      <category term="leecode" scheme="http://yoursite.com/tags/leecode/"/>
    
  </entry>
  
  <entry>
    <title>20191219leecode142快慢指针学习</title>
    <link href="http://yoursite.com/2019/12/19/20191219leecode142%E5%BF%AB%E6%85%A2%E6%8C%87%E9%92%88%E5%AD%A6%E4%B9%A0/"/>
    <id>http://yoursite.com/2019/12/19/20191219leecode142%E5%BF%AB%E6%85%A2%E6%8C%87%E9%92%88%E5%AD%A6%E4%B9%A0/</id>
    <published>2019-12-19T13:25:02.000Z</published>
    <updated>2019-12-19T13:39:55.918Z</updated>
    
    <content type="html"><![CDATA[<h3 id="1-题目"><a href="#1-题目" class="headerlink" title="1 题目"></a>1 题目</h3><p>leecode142，给定一个链表，返回链表开始入环的第一个节点。 如果链表无环，则返回NULL。</p><h3 id="2-hash法与快慢指针法"><a href="#2-hash法与快慢指针法" class="headerlink" title="2 hash法与快慢指针法"></a>2 hash法与快慢指针法</h3><p>快慢指针法有意思的推导：</p><p>x：link起点到入环点距离</p><p>y: 入环点到相遇点距离</p><p>c：circle的长度</p><p>相遇时候，慢指针走了x+n1 c  + y （n1假设走了n1圈），快指针走了2倍(x+ n1c+y)</p><p>快指针比慢指针多走的路程一定是环长度的整数倍，有2(x+ n1c+y) - (x+ n1c+y) = n2 c </p><p>所以又x + y = (n2 - n1) c</p><p>可以相遇时快指针从起点再走（1倍速），慢指针也走，则相遇点就是入环点。</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//  leecode 142 circle link detection</span></span><br><span class="line"></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;stdio.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;set&gt;</span></span></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> <span class="built_in">std</span>;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">ListNode</span>&#123;</span></span><br><span class="line">    <span class="keyword">int</span> val;</span><br><span class="line">    ListNode *next;</span><br><span class="line">    ListNode(<span class="keyword">int</span> x) : val(x), next(<span class="literal">NULL</span>) &#123;&#125;</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="comment">// save elem to set, time O(n) space O(n)</span></span><br><span class="line"><span class="function">ListNode *<span class="title">detectCycle</span><span class="params">(ListNode *head)</span> </span>&#123;</span><br><span class="line">    ListNode* p = head;</span><br><span class="line">    <span class="built_in">set</span>&lt;ListNode*&gt; elem_set;</span><br><span class="line">    <span class="keyword">while</span>(p)&#123;</span><br><span class="line">        <span class="keyword">if</span> (elem_set.<span class="built_in">find</span>(p) != elem_set.<span class="built_in">end</span>()) &#123;</span><br><span class="line">            <span class="keyword">return</span> p;</span><br><span class="line">        &#125;</span><br><span class="line">        elem_set.insert(p);<span class="comment">//O(logN)</span></span><br><span class="line">        p = p-&gt;next;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> <span class="literal">NULL</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">//fast and slow pointer</span></span><br><span class="line"><span class="function">ListNode *<span class="title">detectCycle1</span><span class="params">(ListNode *head)</span></span>&#123;</span><br><span class="line">    ListNode *slow,*fast;</span><br><span class="line">    slow = head;</span><br><span class="line">    fast = head;</span><br><span class="line">    <span class="keyword">while</span> (slow!=<span class="literal">NULL</span> &amp;&amp; fast-&gt;next!=<span class="literal">NULL</span>) &#123;</span><br><span class="line">        slow = slow-&gt;next;</span><br><span class="line">        fast = fast-&gt;next-&gt;next;</span><br><span class="line">        <span class="keyword">if</span> (slow == fast) &#123;</span><br><span class="line">            <span class="comment">//fast pointer go from and start of the link</span></span><br><span class="line">            fast = head;</span><br><span class="line">            <span class="keyword">while</span> (fast != slow) &#123;</span><br><span class="line">                fast = fast-&gt;next;</span><br><span class="line">                slow = slow-&gt;next;</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="keyword">return</span> fast;<span class="comment">//now both pointer is in the start of the citcle</span></span><br><span class="line">            <span class="keyword">break</span>;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> <span class="literal">NULL</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span></span>&#123;</span><br><span class="line">    <span class="comment">//input</span></span><br><span class="line">    ListNode* res;</span><br><span class="line">    ListNode *dummyhead,*head,*temp0,*temp1;</span><br><span class="line">    dummyhead = <span class="keyword">new</span> ListNode(<span class="number">-1</span>);</span><br><span class="line">    head = <span class="keyword">new</span> ListNode(<span class="number">1</span>);</span><br><span class="line">    dummyhead -&gt;next = head;</span><br><span class="line">    temp0 = <span class="keyword">new</span> ListNode(<span class="number">2</span>);</span><br><span class="line">    head-&gt;next = temp0;</span><br><span class="line">    temp1 = <span class="keyword">new</span> ListNode(<span class="number">4</span>);</span><br><span class="line">    temp0-&gt;next = temp1;</span><br><span class="line">    temp1-&gt;next = temp0;</span><br><span class="line">    </span><br><span class="line">    <span class="comment">//algorithm</span></span><br><span class="line">    <span class="comment">//res = detectCycle(head);</span></span><br><span class="line">    res = detectCycle1(head);</span><br><span class="line">    <span class="keyword">if</span> (res == <span class="literal">NULL</span>) &#123;</span><br><span class="line">        <span class="built_in">cout</span>&lt;&lt;<span class="string">"no circle"</span>&lt;&lt;<span class="built_in">endl</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">else</span> <span class="built_in">cout</span>&lt;&lt;res-&gt;val&lt;&lt;<span class="built_in">endl</span>;</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h3 id=&quot;1-题目&quot;&gt;&lt;a href=&quot;#1-题目&quot; class=&quot;headerlink&quot; title=&quot;1 题目&quot;&gt;&lt;/a&gt;1 题目&lt;/h3&gt;&lt;p&gt;leecode142，给定一个链表，返回链表开始入环的第一个节点。 如果链表无环，则返回NULL。&lt;/p&gt;
&lt;h3 id=&quot;
      
    
    </summary>
    
    
      <category term="算法" scheme="http://yoursite.com/categories/%E7%AE%97%E6%B3%95/"/>
    
    
      <category term="leecode" scheme="http://yoursite.com/tags/leecode/"/>
    
  </entry>
  
  <entry>
    <title>20191216字节面试&amp;图论问题回顾</title>
    <link href="http://yoursite.com/2019/12/17/20191216%E5%AD%97%E8%8A%82%E9%9D%A2%E8%AF%95%E4%B8%8E%E5%8D%8E%E4%B8%BA%E5%9B%BE%E8%AE%BA%E9%97%AE%E9%A2%98/"/>
    <id>http://yoursite.com/2019/12/17/20191216%E5%AD%97%E8%8A%82%E9%9D%A2%E8%AF%95%E4%B8%8E%E5%8D%8E%E4%B8%BA%E5%9B%BE%E8%AE%BA%E9%97%AE%E9%A2%98/</id>
    <published>2019-12-17T07:44:14.000Z</published>
    <updated>2019-12-22T07:07:37.282Z</updated>
    
    <content type="html"><![CDATA[<h3 id="1，一面"><a href="#1，一面" class="headerlink" title="1，一面"></a>1，一面</h3><p>一面的面试官超级可爱，很温和，我也太幸运了吧。先让自我介绍，然后问项目，然后出了一个很简单的算法题。</p><h4 id="1-1-图论问题"><a href="#1-1-图论问题" class="headerlink" title="1.1 图论问题"></a>1.1 图论问题</h4><p>项目是我大三做的一个图论赛题，回顾总结一下。</p><p>问题：“服务器选址问题”，从图中选出一些节点安放服务器（图中绿色节点 表示为$S_i$），服务器输出流量供给消费节点（图中红色节点，表示为$C_i$）</p><p>目标：第一要满足每个消费节点的流量需求，第二费用最小。</p><p>约束：每个路径有流量限制$flow_{constrain}$，但上下行都可以。也有流量单位费用$UnitCost$。举例子比如图右上角从1到15节点，留出流量13，则费用是13 * 2 = 26，此条路后面只能再流过16 - 13 = 3的流量了。另外其他限制是90秒内必须输出结果，否则没有成绩，使用内存不超过2GB。</p><p>输出：每条路径，及流过的流量。</p><p><img src="/images/20170305HuaWeiFlow.png" alt="20170305HuaWeiFlow"></p><h4 id="1-2-我的算法"><a href="#1-2-我的算法" class="headerlink" title="1.2 我的算法"></a>1.2 我的算法</h4><h5 id="策略1：选择前n个可以流出带宽最大的节点"><a href="#策略1：选择前n个可以流出带宽最大的节点" class="headerlink" title="策略1：选择前n个可以流出带宽最大的节点"></a>策略1：选择前n个可以流出带宽最大的节点</h5><p>1，选择n个可以流出带宽最大的节点</p><p>step1 计算每个结点可以输出的带宽之和</p><p>step2 排序</p><p>step3 选择前n个，放置服务器</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">getServerlocation</span><span class="params">(Graph&amp; g,<span class="keyword">int</span>* server)</span><span class="comment">//bigest bandwith Id</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">int</span> location=<span class="number">0</span>,i,bandwidth=<span class="number">0</span>,SecondBandWidth=<span class="number">0</span>;</span><br><span class="line">    <span class="keyword">for</span>(i=<span class="number">0</span>;i&lt;g.numV();i++)&#123;</span><br><span class="line">        <span class="keyword">if</span>(g.Bandwidth[i] &gt; bandwidth)&#123;</span><br><span class="line">            SecondBandWidth = bandwidth;</span><br><span class="line">            location = i;</span><br><span class="line">            bandwidth = g.Bandwidth[i];</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">for</span>(i=<span class="number">0</span>;i&lt;g.numC();i++)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="keyword">if</span>(location == server[i])&#123;</span><br><span class="line">            <span class="keyword">for</span>(i=<span class="number">0</span>;i&lt;g.numV();i++)&#123;</span><br><span class="line">                <span class="keyword">if</span>(g.Bandwidth[i] == SecondBandWidth)</span><br><span class="line">                    location = i;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> location;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>step4 用Dijkstra计算服务器到消费节点的最短路径（只根据$flow_{constrain}$来计算），计算这条路径的可以流过的最大流量，分配流量，计算费用。</p><p>Dijkstra算法：</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span>(w=<span class="number">0</span>;w&lt;g.CountsOfConnectNode[v];w++)&#123;</span><br><span class="line"><span class="keyword">if</span> (D[w] &gt; D[v] + g.getUnitConsumeCost(v, w))  计算server到各个点的距离，判断并更新</span><br><span class="line">D[w] = D[v] + g.getUnitConsumeCost(v, w);</span><br></pre></td></tr></table></figure><p>获取本条路径可以流过的最大流量</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">getflow</span><span class="params">(Graph* G,<span class="keyword">int</span> path[maxN][maxN],<span class="keyword">int</span> cn)</span><span class="comment">//get cn-consumenode minflow</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">int</span> flow = <span class="number">0</span>;</span><br><span class="line">    <span class="keyword">int</span> Cnode  = G-&gt;Con_Nodes[cn];</span><br><span class="line">    <span class="keyword">int</span> minflow = INFINITY,i;</span><br><span class="line">    <span class="keyword">for</span>(i=<span class="number">0</span>;i&lt;maxN;i++)&#123;</span><br><span class="line">        <span class="keyword">if</span>(minflow &gt; G-&gt;getWeight(path[Cnode][i], path[Cnode][i+<span class="number">1</span>]))&#123;</span><br><span class="line">            minflow = G-&gt;getWeight(path[Cnode][i], path[Cnode][i+<span class="number">1</span>]);</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">if</span>(path[Cnode][i+<span class="number">2</span>] == <span class="number">-1</span>)&#123;</span><br><span class="line">            flow = minflow;</span><br><span class="line">            <span class="keyword">break</span>;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">if</span>(flow &gt; G-&gt;Demand[cn])</span><br><span class="line">        flow = G-&gt;Demand[cn];</span><br><span class="line">    <span class="keyword">return</span> flow;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>更新图和消费节点的流量需求。</p><p>反过来消费点去找离他最近的服务器节点，分配流量。</p><h5 id="策略2：实在不行选择与消费点的直连点放服务器。"><a href="#策略2：实在不行选择与消费点的直连点放服务器。" class="headerlink" title="策略2：实在不行选择与消费点的直连点放服务器。"></a>策略2：实在不行选择与消费点的直连点放服务器。</h5><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/* server is in the consumenode */</span></span><br><span class="line">   <span class="keyword">for</span>(i=<span class="number">0</span>;i&lt;graph.numC();i++)&#123;</span><br><span class="line">       <span class="keyword">if</span>(server[ser<span class="number">-1</span>] == graph.Con_Nodes[i])&#123;</span><br><span class="line">           pathpath[pathi][<span class="number">0</span>] = server[ser<span class="number">-1</span>];  <span class="comment">//add to the answer</span></span><br><span class="line">           pathpath[pathi][<span class="number">1</span>] = i;</span><br><span class="line">           pathpath[pathi][<span class="number">2</span>] = graph.Demand[i];</span><br><span class="line">           pathi++;</span><br><span class="line">           graph.Demand[i] = <span class="number">0</span>;</span><br><span class="line">       &#125;</span><br><span class="line">   &#125;</span><br></pre></td></tr></table></figure><h4 id="1-3-其他方法"><a href="#1-3-其他方法" class="headerlink" title="1.3 其他方法"></a>1.3 其他方法</h4><p>整数规划模型</p><p>启发式算法（模拟退火，遗传算法，去网上找了资料学了点基础）</p><h4 id="当年的反思"><a href="#当年的反思" class="headerlink" title="当年的反思"></a>当年的反思</h4><p><img src="/images/20191218HuaWeiRes.jpg" alt="20191218HuaWeiRes"></p><p>1、一个类里面空间是有限的，如果开了几个1000*1000的二维数组是不行的，要么static，要么设置全局。</p><p>2、在Dijkstra 里CountsOfConnectNode不能因为单边减少而减一，因为单边减少就减一的话会造成无法访问一些边，因为我是先遍历的在判断的周边路径是否存在，即weight &gt; 0。</p><p>3、一些存DotId的数组我初始化为了-1，其实-1是很容易造成下标越界的，但本来dot的范围是0~maxN，所以造成后面很多的判断 ！= -1 ，希望大家引以为戒。（因为-1 ，我的graph里的成员变量servercost竟然从100变成了-1，就是因为-1下标的范围导致内存访问异常，数据被修改，当时真是急哭我了）</p><p>4、记住所有变量定义的时候一定初始化，否则为任意值的话会造成不可知的错误，只能一直debug一步步找变量的变化，真是心累。</p><p>5、如果你的数据很多，请注释每个的含义，包括下标，否则你的队友会看不懂你的代码，自己写一写的就会弄混。</p><p>6、算法上的缺陷</p><p>没有反馈：一直计算的出来的结果，没有经过比较选择这是缺乏了优化的过程的。应该要一直迭代，随机取、放一些服务器后就算一遍最短路径和成本进行比较取优。其实我的代码跑完整个用的时间是ms级的，那么其实还要很多时间可以进行计算。因为最后来不及了也就没有做，自然成本高。</p><p>7、团队分工：队友要充分合作（一个人再强大真的比不上三个臭皮匠）、分工写任务，一定要充分相信对方。队长要想好整体，再把模块分开写，把需求明确，免得最后代码合并要哭。</p><p>8、编程基本功：编程基本功要多写多练才扎实，不然写这样的复杂稍大的程序就很容易出现一些低级错误</p><p>9、多去学习大佬怎么做的，站在前人大佬的基础上才不会自己太犯傻，至少基本的方向不会错！</p><h3 id="2，算法题"><a href="#2，算法题" class="headerlink" title="2，算法题"></a>2，算法题</h3><p>1，写出二叉树的最短路径长度。长度 = 路径上节点的值的和。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">int MaxPath(TreeNode* root)&#123;</span><br><span class="line">if (root -&gt; NULL) return 0;</span><br><span class="line">else return root-&gt;val + max(MaxPath(root-&gt;left),MaxPath(root-&gt;right));</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>2，给出一个数组[1 3 2 6 5 7 10]，找出后面的比他大的第一个值，返回下标，答案 [1 3 3 5 5 6 -1]。</p><p>思路：倒过来遍历，取一个最小的stack</p><p>倒着遍历，维护一个递减的stack(top保持最小）。先10和其index绑定入stack，然后轮到7，判断stack top，若大于7就把stack top的数的index返回，否则弹出stack top，直到找到大于7或者stack弹空，若弹空则返回-1.然后把7绑定index压到stack里。</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;stdio.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;stack&gt;</span></span></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> <span class="built_in">std</span>;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">ListNode</span>&#123;</span></span><br><span class="line">    <span class="keyword">int</span> value;</span><br><span class="line">    <span class="keyword">int</span> index;</span><br><span class="line">    ListNode(<span class="keyword">int</span> a,<span class="keyword">int</span> b)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="keyword">this</span>-&gt;value=a;</span><br><span class="line">        <span class="keyword">this</span>-&gt;index=b;</span><br><span class="line">    &#125;;</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span></span>&#123;</span><br><span class="line">    <span class="built_in">stack</span>&lt;ListNode&gt; stk;</span><br><span class="line">    <span class="built_in">stack</span>&lt;<span class="keyword">int</span>&gt; res;</span><br><span class="line">    <span class="keyword">int</span> N; <span class="comment">//N &gt; 0</span></span><br><span class="line">    <span class="built_in">cin</span>&gt;&gt;N;</span><br><span class="line">    <span class="keyword">int</span> arr[N];</span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> i=<span class="number">0</span>; i&lt;N; i++) &#123;</span><br><span class="line">        <span class="built_in">cin</span>&gt;&gt;arr[i];</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    <span class="comment">//algotithm</span></span><br><span class="line">    res.push(<span class="number">-1</span>);</span><br><span class="line">    <span class="function">ListNode <span class="title">temp</span><span class="params">(arr[N<span class="number">-1</span>],N<span class="number">-1</span>)</span></span>;</span><br><span class="line">    stk.push(temp);<span class="comment">//put last value,arr[N-1]</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> i=N<span class="number">-1</span>; i&gt;=<span class="number">0</span>; i--) &#123;</span><br><span class="line">        <span class="keyword">while</span> ( !stk.empty()) &#123;</span><br><span class="line">            <span class="keyword">if</span> (arr[i] &lt; stk.top().value) &#123;<span class="comment">//compare with stack top value</span></span><br><span class="line">                res.push(stk.top().index);<span class="comment">//remember the result</span></span><br><span class="line">                <span class="function">ListNode <span class="title">temp</span><span class="params">(arr[i],i)</span></span>;</span><br><span class="line">                stk.push(temp);<span class="comment">//put current arr[i]</span></span><br><span class="line">                <span class="keyword">break</span>;</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="keyword">else</span>&#123;</span><br><span class="line">                stk.pop();</span><br><span class="line">                <span class="keyword">if</span> (stk.empty()) &#123;</span><br><span class="line">                    res.push(<span class="number">-1</span>); <span class="comment">//no bigger data, so res is -1</span></span><br><span class="line">                    <span class="function">ListNode <span class="title">temp</span><span class="params">(arr[i],i)</span></span>; <span class="comment">//put this value in it</span></span><br><span class="line">                    stk.push(temp);</span><br><span class="line">                    <span class="keyword">break</span>;</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> i=<span class="number">0</span>; i&lt;N; i++) &#123;<span class="comment">//print the result</span></span><br><span class="line">        <span class="built_in">cout</span>&lt;&lt;res.top()&lt;&lt;<span class="built_in">endl</span>;</span><br><span class="line">        res.pop();</span><br><span class="line">    &#125;</span><br><span class="line">  <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h3 id=&quot;1，一面&quot;&gt;&lt;a href=&quot;#1，一面&quot; class=&quot;headerlink&quot; title=&quot;1，一面&quot;&gt;&lt;/a&gt;1，一面&lt;/h3&gt;&lt;p&gt;一面的面试官超级可爱，很温和，我也太幸运了吧。先让自我介绍，然后问项目，然后出了一个很简单的算法题。&lt;/p&gt;
&lt;h4 id=
      
    
    </summary>
    
    
      <category term="算法" scheme="http://yoursite.com/categories/%E7%AE%97%E6%B3%95/"/>
    
    
      <category term="面试" scheme="http://yoursite.com/tags/%E9%9D%A2%E8%AF%95/"/>
    
      <category term="算法" scheme="http://yoursite.com/tags/%E7%AE%97%E6%B3%95/"/>
    
  </entry>
  
  <entry>
    <title>20191210Skyline源码阅读</title>
    <link href="http://yoursite.com/2019/12/10/20191210Skyline%E6%BA%90%E7%A0%81%E9%98%85%E8%AF%BB/"/>
    <id>http://yoursite.com/2019/12/10/20191210Skyline%E6%BA%90%E7%A0%81%E9%98%85%E8%AF%BB/</id>
    <published>2019-12-10T01:32:49.000Z</published>
    <updated>2019-12-31T10:33:24.904Z</updated>
    
    <content type="html"><![CDATA[<h3 id="1-基类结构"><a href="#1-基类结构" class="headerlink" title="1 基类结构"></a>1 基类结构</h3><p>首先看异常检测基类base.py，所有检测器都是由它继承而来：</p><p>1，init() ：初始化dataSet，probationaryPercent 数据的最初一部分数据不做测试。inputMin, inputMax初始化最大最小值。</p><p>2，initialize()：多进程问题。进程池pool（它默认调用的是CPU的核数）</p><p>3，handleRecord(): 返回每一个时间点的异常分数值，Returns a list [anomalyScore, *]。这个函数子类必须继承。</p><p>4，getAdditionalHeaders()：如HTM检测器里会添加’anomalyscore’ , ‘rawscore’。添加并返回列名的，run函数中调用它拼接最后返回的dataframe。</p><p>5，detectDataSet(): 在运行给定检测器的每个检测器进程中调用的函数。参数 (i, detectorInstance, detectorName, labels, outputDir, relativePath) = args，主要是创建保存文件的路径，调用detectorInstance.initialize()，results = detectorInstance.run() </p><p>6，run()：为整个dataSet打分并返回结果（dataframe格式）</p><h3 id="2-Etsy的Skyline算法"><a href="#2-Etsy的Skyline算法" class="headerlink" title="2 Etsy的Skyline算法"></a>2 Etsy的Skyline算法</h3><p>继承异常检测器基类。另外它的算法是根据几个小算法各自的评分进行平均投票得到。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">self.algorithms =   [median_absolute_deviation,</span><br><span class="line">                     first_hour_average,</span><br><span class="line">                     stddev_from_average,</span><br><span class="line">                     stddev_from_moving_average,</span><br><span class="line">                     mean_subtraction_cumulation,</span><br><span class="line">                     least_squares,</span><br><span class="line">                     histogram_bins]</span><br></pre></td></tr></table></figure><h5 id="median-absolute-deviation"><a href="#median-absolute-deviation" class="headerlink" title="median_absolute_deviation"></a>median_absolute_deviation</h5><p>计算数据的中位数，偏差 = 每个值-中位数，得到偏差中位数</p><script type="math/tex; mode=display">\mathrm{MAD}=\operatorname{median}\left(\left|X_{i}-\operatorname{median}(X)\right|\right)</script><p>MAD对数据集中的异常值比标准偏差更具弹性。在标准偏差中，与均值的距离的平方，较大的异常值会影响更大。可以通过判断一个点的偏差是否过于偏离MAD来判断异常，此处是如果偏差6倍大于中位数，则判断为异常。</p><h5 id="first-hour-average"><a href="#first-hour-average" class="headerlink" title="first_hour_average"></a>first_hour_average</h5><p>上一天的这个时间段1h的均值是$mean$，标准差是$std$，如果$|X_t - mean| &gt; 3 * std$ 则是异常。</p><h5 id="stddev-from-average"><a href="#stddev-from-average" class="headerlink" title="stddev_from_average"></a>stddev_from_average</h5><p>值减去移动平均值大于平均值的三个标准偏差则为异常。</p><script type="math/tex; mode=display">|X_t - mean| > 3 * std</script><h5 id="stddev-from-moving-average"><a href="#stddev-from-moving-average" class="headerlink" title="stddev_from_moving_average"></a>stddev_from_moving_average</h5><p>值减去指数加权移动平均值大于平均值的三个标准偏差则为异常。</p><p>expAvg = series.ewm().mean()</p><p>stdDev = series.ewm().std()</p><script type="math/tex; mode=display">| X_t - expAvg | > 3 * stdDev</script><h5 id="mean-subtraction-cumulation"><a href="#mean-subtraction-cumulation" class="headerlink" title="mean_subtraction_cumulation"></a>mean_subtraction_cumulation</h5><p>从每个数据源点减去过去历史平均值之后，如果该序列中下一个数据点的值比累积项中的三个标准差远，则该时间序列是异常的。</p><h5 id="least-squares"><a href="#least-squares" class="headerlink" title="least_squares"></a>least_squares</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">#X——代表时间 timestamp，Y——代表 value</span><br><span class="line">results = np.linalg.lstsq(A, Y)</span><br><span class="line">residual = results[1] #残差</span><br><span class="line">m, c = np.linalg.lstsq(A, Y)[0] #斜率与截距</span><br><span class="line">  for i, value in enumerate(y):</span><br><span class="line">    projected = m * X[i] + c</span><br><span class="line">    error = value - projected</span><br><span class="line">    errors.append(error)</span><br></pre></td></tr></table></figure><p>最后点投影到最小二乘上误差大于所有误差的std的3sigma时，判断为异常。</p><script type="math/tex; mode=display">Error_t > ErrorsStd</script><h5 id="histogram-bins"><a href="#histogram-bins" class="headerlink" title="histogram_bins"></a>histogram_bins</h5><p>最后时间点的值落入带有少于threshold个其他数据点的直方图bin中，则时间序列是异常的。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h3 id=&quot;1-基类结构&quot;&gt;&lt;a href=&quot;#1-基类结构&quot; class=&quot;headerlink&quot; title=&quot;1 基类结构&quot;&gt;&lt;/a&gt;1 基类结构&lt;/h3&gt;&lt;p&gt;首先看异常检测基类base.py，所有检测器都是由它继承而来：&lt;/p&gt;
&lt;p&gt;1，init() ：初始化da
      
    
    </summary>
    
    
      <category term="AIOps" scheme="http://yoursite.com/categories/AIOps/"/>
    
    
      <category term="AIOps" scheme="http://yoursite.com/tags/AIOps/"/>
    
      <category term="机器学习" scheme="http://yoursite.com/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>NAB源码阅读系列1</title>
    <link href="http://yoursite.com/2019/12/04/NAB%E6%BA%90%E7%A0%81%E9%98%85%E8%AF%BB%E7%B3%BB%E5%88%971/"/>
    <id>http://yoursite.com/2019/12/04/NAB%E6%BA%90%E7%A0%81%E9%98%85%E8%AF%BB%E7%B3%BB%E5%88%971/</id>
    <published>2019-12-04T02:04:59.000Z</published>
    <updated>2019-12-09T13:32:32.115Z</updated>
    
    <content type="html"><![CDATA[<h3 id="1-NAB的基本框架"><a href="#1-NAB的基本框架" class="headerlink" title="1 NAB的基本框架"></a>1 NAB的基本框架</h3><p>NAB里包括了几个在线时间序列异常检测器，包括了几组异常检测数据集，包括了阈值优化，评分算法，归一化检测器分数等内容，包括了NAB结果及可视化等内容。</p><h3 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h3><p>1, Ahmad S, Lavin A, Purdy S, et al. Unsupervised real-time anomaly detection for streaming data[J]. Neurocomputing, 2017, 262: 134-147.</p><p>2, Lavin A, Ahmad S. Evaluating Real-Time Anomaly Detection Algorithms—The Numenta Anomaly Benchmark[C]//2015 IEEE 14th International Conference on Machine Learning and Applications (ICMLA). IEEE, 2015: 38-44.</p><p>3, <a href="https://github.com/numenta/NAB" target="_blank" rel="noopener">https://github.com/numenta/NAB</a> NAB的开源库里有很多说明，以及白皮书</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h3 id=&quot;1-NAB的基本框架&quot;&gt;&lt;a href=&quot;#1-NAB的基本框架&quot; class=&quot;headerlink&quot; title=&quot;1 NAB的基本框架&quot;&gt;&lt;/a&gt;1 NAB的基本框架&lt;/h3&gt;&lt;p&gt;NAB里包括了几个在线时间序列异常检测器，包括了几组异常检测数据集，包括了阈
      
    
    </summary>
    
    
      <category term="AIOps" scheme="http://yoursite.com/categories/AIOps/"/>
    
    
      <category term="异常检测" scheme="http://yoursite.com/tags/%E5%BC%82%E5%B8%B8%E6%A3%80%E6%B5%8B/"/>
    
      <category term="时间序列" scheme="http://yoursite.com/tags/%E6%97%B6%E9%97%B4%E5%BA%8F%E5%88%97/"/>
    
      <category term="智能运维" scheme="http://yoursite.com/tags/%E6%99%BA%E8%83%BD%E8%BF%90%E7%BB%B4/"/>
    
      <category term="Python" scheme="http://yoursite.com/tags/Python/"/>
    
  </entry>
  
  <entry>
    <title>mac上hexo的mathjax配置</title>
    <link href="http://yoursite.com/2019/11/26/mac%E4%B8%8Ahexo%E7%9A%84mathjax%E9%85%8D%E7%BD%AE/"/>
    <id>http://yoursite.com/2019/11/26/mac%E4%B8%8Ahexo%E7%9A%84mathjax%E9%85%8D%E7%BD%AE/</id>
    <published>2019-11-26T03:50:08.000Z</published>
    <updated>2019-12-10T01:46:28.124Z</updated>
    
    <content type="html"><![CDATA[<p>博文中要写公式是难免的，因为配置hexo支持数学公式是必要的。 Next 主题提供了两个渲染引擎，分别是 mathjax 和 katex，后者相对前者来说渲染速度更快，而且支持更丰富的公式。我这里hexo是4.0版本了，因此又折腾了下。</p><h6 id="1，更改next下的config"><a href="#1，更改next下的config" class="headerlink" title="1，更改next下的config"></a>1，更改next下的config</h6><p>配置next主题里的_config如下，只需要改一个地方就是mathjax的enable为true。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"># Math Formulas Render Support</span><br><span class="line">math:</span><br><span class="line">  # Default (true) will load mathjax / katex script on demand.</span><br><span class="line">  # That is it only render those page which has `mathjax: true` in Front-matter.</span><br><span class="line">  # If you set it to false, it will load mathjax / katex srcipt EVERY PAGE.</span><br><span class="line">  per_page: true</span><br><span class="line"></span><br><span class="line">  # hexo-renderer-pandoc (or hexo-renderer-kramed) required for full MathJax support.</span><br><span class="line">  mathjax:</span><br><span class="line">    enable: true</span><br><span class="line">    # See: https://mhchem.github.io/MathJax-mhchem/</span><br><span class="line">    mhchem: false</span><br></pre></td></tr></table></figure><h6 id="2-去掉hexo自带的数学渲染"><a href="#2-去掉hexo自带的数学渲染" class="headerlink" title="2, 去掉hexo自带的数学渲染"></a>2, 去掉hexo自带的数学渲染</h6><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">npm uninstall hexo-renderer-marked --save</span><br><span class="line">npm install hexo-renderer-kramed --save</span><br></pre></td></tr></table></figure><p>在修改下源文件。打开<code>node_modules/hexo-renderer-kramed/lib/renderer.js</code>，将</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">// Change inline math rule</span><br><span class="line">function formatText(text) &#123;</span><br><span class="line">    // Fit kramed&apos;s rule: $$ + \1 + $$</span><br><span class="line">    return text.replace(/`\$(.*?)\$`/g, &apos;$$$$$1$$$$&apos;);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>改为：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">// Change inline math rule</span><br><span class="line">function formatText(text) &#123;</span><br><span class="line">    return text;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>卸载hexo-math，安装新的。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">npm uninstall hexo-math --save</span><br><span class="line">npm install hexo-renderer-mathjax --save</span><br></pre></td></tr></table></figure><p>在修改源文件，打开<code>node_modules/hexo-renderer-mathjax/mathjax.html</code>，将最后一句script改为：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&lt;script src=&quot;https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML&quot;&gt;&lt;/script&gt;</span><br></pre></td></tr></table></figure><p>打开<code>node_modules/kramed/lib/rules/inline.js</code> : </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">// escape: /^\\([\\`*&#123;&#125;\[\]()#$+\-.!_&gt;])/,  注释掉改为下面一句</span><br><span class="line">escape: /^\\([`*\[\]()# +\-.!_&gt;])/,</span><br></pre></td></tr></table></figure><p>下面的em渲染也改了:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">// em: /^\b_((?:__|[\s\S])+?)_\b|^\*((?:\*\*|[\s\S])+?)\*(?!\*)/, 注释掉改为下面一句</span><br><span class="line">em: /^\*((?:\*\*|[\s\S])+?)\*(?!\*)/,</span><br></pre></td></tr></table></figure><h6 id="3，开启bolg下的config支持"><a href="#3，开启bolg下的config支持" class="headerlink" title="3，开启bolg下的config支持"></a>3，开启bolg下的config支持</h6><p>在末尾添加内容。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">mathjax:</span><br><span class="line">    enable: true</span><br></pre></td></tr></table></figure><p>就可以了，鉴于之前的博客可能有些老了，配置了半天就记录下。</p><h6 id="4，最后自己在写bolg的时候头部加上mathjax-true，表示本文要数学公式渲染。"><a href="#4，最后自己在写bolg的时候头部加上mathjax-true，表示本文要数学公式渲染。" class="headerlink" title="4，最后自己在写bolg的时候头部加上mathjax: true，表示本文要数学公式渲染。"></a>4，最后自己在写bolg的时候头部加上mathjax: true，表示本文要数学公式渲染。</h6>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;博文中要写公式是难免的，因为配置hexo支持数学公式是必要的。 Next 主题提供了两个渲染引擎，分别是 mathjax 和 katex，后者相对前者来说渲染速度更快，而且支持更丰富的公式。我这里hexo是4.0版本了，因此又折腾了下。&lt;/p&gt;
&lt;h6 id=&quot;1，更改ne
      
    
    </summary>
    
    
      <category term="配置" scheme="http://yoursite.com/categories/%E9%85%8D%E7%BD%AE/"/>
    
    
      <category term="配置" scheme="http://yoursite.com/tags/%E9%85%8D%E7%BD%AE/"/>
    
  </entry>
  
  <entry>
    <title>Numenta的HTM简介</title>
    <link href="http://yoursite.com/2019/11/25/Numenta%E7%9A%84HTM%E7%AE%80%E4%BB%8B/"/>
    <id>http://yoursite.com/2019/11/25/Numenta%E7%9A%84HTM%E7%AE%80%E4%BB%8B/</id>
    <published>2019-11-25T08:26:49.000Z</published>
    <updated>2019-12-04T04:18:05.801Z</updated>
    
    <content type="html"><![CDATA[<h4 id="1-Numenta的HTM简介"><a href="#1-Numenta的HTM简介" class="headerlink" title="1, Numenta的HTM简介"></a>1, Numenta的HTM简介</h4><p>Hierarchical Temporal Memeory(HTM,层级时间记忆，皮质学习) 是一种基于脑神经科学来模拟大脑进行学习和信息处理的神经网络。新皮质就是大脑里褶皱的皮层部分（图1），这只有哺乳动物有。将皮层纵向切开，不论是视觉还是听觉部分，切开后的结构是相似的（图2），很有可能大脑处理不同信息的方法是类似的。</p><p><img src="/images/20191125head.jpg" width="250"/><br><img src="/images/20191125cells.png" width="200"/> </p><p><center>图1 大脑皮层，图2 细胞图</center><br>新皮质分化为很多个区域（region，图3），这些区域通过神经纤维连接。这些区域以层次结构的方式连接在一起。低层级信息收集基础信号，经过不同层级逐渐加工，提取并理解更抽象信息，更高级的话或许可以关联到想法、事物活动等信息。这个有点类似卷积神经网络，低层级的网络提取图像边界等信息，高层级的网络识别物体类型等等。</p><p><img src="/images/20191125HierarchicalMode1.png" width="400"/></p><p><center> 图 3 HTM分层示意图</center><br>目前，<strong>Numenta的HTM设计介绍讲解主要针对一个区域，即一层（图3，如黄色层），说明其数据输入方式，数据表征方式，神经元激活，以及时间记忆表示方式</strong>。HTM大概的原理是，首先将输入的数据编码为0、1稀疏数组，将这些稀疏数组经过空间池化转换为稀疏分布表征（SDR），然后时序记忆，建立突触，存储信息，进行预测等。</p><h4 id="2-数据输入"><a href="#2-数据输入" class="headerlink" title="2, 数据输入"></a>2, 数据输入</h4><p>数据输入一般有数字，日期，温度等，将这些数据编码为01稀疏数组（bit数组）。这在计算机领域十分常见，如一个字符的ASCⅡ表示，使用8bit表示的。n个bit可以表示$2^n$容量（capacity）的信息，bit数组可以有许多运算，与或非与异或等等。</p><p><img src="/images/20191125featureRepresentation1.png" width="400"/></p><p>在HTM里，稀疏的每一个1可能表示了一个信息。在通过稀疏bit数组的压缩存储（只存1的下标位置），可以表示非常多的数据信息了。</p><h4 id="3-空间池化Spatial-Pooler"><a href="#3-空间池化Spatial-Pooler" class="headerlink" title="3, 空间池化Spatial Pooler"></a>3, 空间池化Spatial Pooler</h4><h5 id="3-1-稀疏分布表征-SDR"><a href="#3-1-稀疏分布表征-SDR" class="headerlink" title="3.1 稀疏分布表征 SDR"></a>3.1 稀疏分布表征 SDR</h5><p>稀疏分布表征（SDR）是空间池化的结果，通俗来看有点像大脑的数据结构，我们先看看SDR的一些特性，如图。计算SDR的容量:</p><script type="math/tex; mode=display">capacity = \left( \begin{array} { c } { n } \\ { w } \end{array} \right) = \frac { n! } { w! ( n - w )! } = C_n^w （组合数）</script><p>也就是说可以表示这么多的信息量。</p><p><img src="/images/20191125SDR_Define.png" width="400"/></p><p>1，SDR的一些基本运算。overlap交集，两个SDR交起来，相同的激活的bit越多，表明这俩SDR越相似。判断俩SDR是否匹配，可以设置一定的阈值。当俩SDR overlap之后，交集bit  $&gt;=\theta$ (阈值)，则俩SDR匹配。</p><p>2，SDR的噪声容忍度（noise tolerant）强。在下图中，选取29%的比例翻转bit的值，对比两个SDR，重叠分数为30。当30大于等于$\theta=30$ 则匹配。意思是说如果俩SDR是原本一致，就算其中一个SDR不完全准确有噪声，则还是会匹配上的。当然也有可能确实两SDR不一致，但又因为噪声导致其匹配上了，这样的误报可能有，但是概率很低 $FP = 交集的基数 / 原始SDR的n w的组合数 $ </p><p><img src="/images/20191125NoiseTolerant.jpg" width="400"/></p><h5 id="3-2-SDR的重叠集"><a href="#3-2-SDR的重叠集" class="headerlink" title="3.2 SDR的重叠集"></a>3.2 SDR的重叠集</h5><p>如果俩同样大小的SDR（即$n,w$ 分别相等），所有bit匹配，则匹配的SDR必然跟原SDR一模一样，就只有一个。那如果降低匹配阈值 $\theta$ ，当相同激活的bit数目为$\theta$时，可以有多少个SDR与原SDR相匹配呢？ 这是个排列组合问题。</p><script type="math/tex; mode=display">\left|\Omega(n, w, \theta)\right|=\left(\begin{array}{c}{w} \\ {\theta}\end{array}\right) \times\left(\begin{array}{l}{n-w} \\ {w-\theta}\end{array}\right)</script><p>相匹配的SDR，左边从原SDR里$w$里选出$\theta$个bit来激活，这是俩SDR相同激活的bit。右边从原SDR里没有激活的$n-w$ 个bit里选出 $w-\theta$ 来激活即可。若 $n=600, w=40, \theta = 39$，算一算可以有 $40 * 560$个不同的SDR与原SDR匹配，是不是很多呀。</p><p>这有个好处就是，SDR可以表示很多相似的信息，而且可以直接通过俩SDR的交集来判断是否相似，误报率也很低。</p><h5 id="3-3-SDR栈"><a href="#3-3-SDR栈" class="headerlink" title="3.3 SDR栈"></a>3.3 SDR栈</h5><p>随着时间序列值逐步产生，即SDR也逐步产生。我们模拟看到SDR进行匹配的过程。new SDR与栈里的SDRs匹配，看看之前是不是见到过。匹配的SDR会有很多重叠的bit。</p><p><img src="/images/20191204SDR_Stack.jpg" width="400"/></p><p>为了加快计算，之前的所有SDR采用Union合并到一起进行匹配。其实由于$n$很大，错误匹配的概率还是很小的。</p><h4 id="5-时序记忆-Temporal-Memory"><a href="#5-时序记忆-Temporal-Memory" class="headerlink" title="5, 时序记忆 Temporal Memory"></a>5, 时序记忆 Temporal Memory</h4><h4 id="6-总结"><a href="#6-总结" class="headerlink" title="6, 总结"></a>6, 总结</h4><h4 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h4><p>1，<a href="https://www.bilibili.com/video/av35735228?from=search&amp;seid=7001690129614399170" target="_blank" rel="noopener">bilibili的翻译HTM school</a></p><p>2，<a href="https://numenta.org/htm-school/" target="_blank" rel="noopener">numenta的YouTube视频</a></p><p>3， Ahmad S, Lavin A, Purdy S, et al. Unsupervised real-time anomaly detection for streaming data[J]. Neurocomputing, 2017, 262: 134-147.</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h4 id=&quot;1-Numenta的HTM简介&quot;&gt;&lt;a href=&quot;#1-Numenta的HTM简介&quot; class=&quot;headerlink&quot; title=&quot;1, Numenta的HTM简介&quot;&gt;&lt;/a&gt;1, Numenta的HTM简介&lt;/h4&gt;&lt;p&gt;Hierarchical Tem
      
    
    </summary>
    
    
      <category term="AIOps" scheme="http://yoursite.com/categories/AIOps/"/>
    
    
      <category term="AIOps" scheme="http://yoursite.com/tags/AIOps/"/>
    
      <category term="神经网络" scheme="http://yoursite.com/tags/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
    
  </entry>
  
  <entry>
    <title>[paper]2019/11/06AIOps: Real-World Challenges and Research Innovations</title>
    <link href="http://yoursite.com/2019/11/06/paper-2019-11-06AIOps-Real-World-Challenges-and-Research-Innovations/"/>
    <id>http://yoursite.com/2019/11/06/paper-2019-11-06AIOps-Real-World-Challenges-and-Research-Innovations/</id>
    <published>2019-11-06T12:56:04.000Z</published>
    <updated>2019-11-06T12:56:38.592Z</updated>
    
    <content type="html"><![CDATA[<h3 id="论文信息"><a href="#论文信息" class="headerlink" title="论文信息"></a>论文信息</h3><p>论文名字：AIOps: Real-World Challenges and Research Innovations<br>引用：Dang Y, Lin Q, Huang P. AIOps: real-world challenges and research innovations[C]//Proceedings of the 41st International Conference on Software Engineering: Companion Proceedings. IEEE Press, 2019: 4-5.</p><h3 id="AIOps定义"><a href="#AIOps定义" class="headerlink" title="AIOps定义"></a>AIOps定义</h3><p>智能运维的定义：通过AI与ML有效构建运维应用 AIOps is about empowering software and service engineers (e.g., developers, program managers, support engineers, site reliability engineers) to efficiently and effectively build and operate online services and applications at scale with artificial intelligence (AI) and machine learning (ML) techniques. </p><p>DevOps 连续开发部署应用（来源于 G. Kim, P. Debois, et al, “The DevOps Handbook: How to Create World- Class Agility, Reliability, and Security in Technology Organizations”, IT Revolution Press, Oct. 2016）</p><h3 id="AIOps的三个目标"><a href="#AIOps的三个目标" class="headerlink" title="AIOps的三个目标"></a>AIOps的三个目标</h3><p>1，服务智能化<br>及时观察多方面变化，质量下降，成本增加，工作量增加等，基于AIOps的服务还可以根据其历史行为，工作量模式和基础来预测其未来状态。根据状态自我调整，trigger self-adaption or auto-healing behaviors of a service, with low human intervention.</p><p>思考：要监控性能，监控反应时间，问题调整策略（自动化调整）</p><p>2，较高的客户满意度<br>具有内置智能的服务可以了解客户的使用行为，并采取积极的行动来提高客户满意度。 例如，服务可以自动向客户推荐调整建议，以使其获得最佳性能（例如，调整配置，冗余级别，资源分配）</p><p>思考：网络不好的话如何自动调整？</p><p>3，高工程生产率<br> 工程师和操作员免于繁琐的工作，例如（1）从各种来源手动收集信息以调查问题； （2）解决重复出现的问题。 工程师和操作人员还可以使用AI / ML技术来学习系统行为的模式，预测服务行为和客户活动的未来，以进行必要的体系结构更改和服务适应策略更改等。</p><h3 id="challenges"><a href="#challenges" class="headerlink" title="challenges"></a>challenges</h3><p>整体思考，充足理解系统<br>工程架构转变 the AIOps engineering principles should include data/label quality monitoring and assurances, continuous model-quality validation, and actionability of insights.<br>缺乏label，极端失衡，数量太少，噪声程度高等，监督或半监督模型<br>组件服务之间的复杂依存关系</p><p>思考：还有服务变更带来的问题，新学习吗？<br>实时数据大量产生，怎么利用?</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h3 id=&quot;论文信息&quot;&gt;&lt;a href=&quot;#论文信息&quot; class=&quot;headerlink&quot; title=&quot;论文信息&quot;&gt;&lt;/a&gt;论文信息&lt;/h3&gt;&lt;p&gt;论文名字：AIOps: Real-World Challenges and Research Innovations&lt;br&gt;
      
    
    </summary>
    
    
      <category term="AIOps" scheme="http://yoursite.com/categories/AIOps/"/>
    
    
      <category term="AIOps" scheme="http://yoursite.com/tags/AIOps/"/>
    
      <category term="论文综述" scheme="http://yoursite.com/tags/%E8%AE%BA%E6%96%87%E7%BB%BC%E8%BF%B0/"/>
    
  </entry>
  
  <entry>
    <title>ubuntu16配置GPU深度学习环境、CUDA、cuNDD等</title>
    <link href="http://yoursite.com/2019/11/06/ubuntu16%E9%85%8D%E7%BD%AEGPU%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%8E%AF%E5%A2%83%E3%80%81CUDA%E3%80%81cuNDD%E7%AD%89/"/>
    <id>http://yoursite.com/2019/11/06/ubuntu16%E9%85%8D%E7%BD%AEGPU%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%8E%AF%E5%A2%83%E3%80%81CUDA%E3%80%81cuNDD%E7%AD%89/</id>
    <published>2019-11-06T03:15:21.000Z</published>
    <updated>2019-12-25T04:10:59.719Z</updated>
    
    <content type="html"><![CDATA[<h3 id="1、准备"><a href="#1、准备" class="headerlink" title="1、准备"></a>1、准备</h3><ol><li>请先看好各种软件的版本对应要求，这仨一定要对应好。<pre><code>  [Tensorflow不同版本要求与CUDA及CUDNN版本对应关系](https://blog.csdn.net/omodao1/article/details/83241074)</code></pre></li><li><p>知道要下哪些版本了，就预先做好各种软件下载工作。<br> 首先下载好英伟达的驱动 <a href="https://www.nvidia.cn/Download/index.aspx?lang=cn" target="_blank" rel="noopener">NVIDIA驱动下载</a><br> 注意！！！下载好跟自己显卡对应的驱动。显卡的产品类型、系列那些如果之前已经装好了驱动，则可以通过命令 nvidia-smi查询到。没有装刚买来就自己查。<br><img src="https://img-blog.csdnimg.cn/20190519153242367.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzMyNjczNDUz,size_1,color_FFFFFF,t_70" alt="我的显卡驱动"><br>即使你的机器之前已经装过驱动，那也最好重新装一遍驱动，因为那个CUDA一定要对应起来。不然后面有坑！</p><p>下载CUDA，链接 <a href="https://developer.nvidia.com/cuda-toolkit-archive" target="_blank" rel="noopener">cuda-toolkit-archive</a><br><img src="https://img-blog.csdnimg.cn/20190519154540803.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzMyNjczNDUz,size_1,color_FFFFFF,t_70" alt="下载CUDA9.0版本"><br>请注意这里一定要选择下载runfilw文件，不是deb！，不然会覆盖之前的显卡驱动带来问题。<br><img src="https://img-blog.csdnimg.cn/20190519154709423.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzMyNjczNDUz,size_1,color_FFFFFF,t_70" alt="对应操作系统下载CUDA"><br>最后下载cuDNN，<a href="https://developer.nvidia.com/rdp/cudnn-archive" target="_blank" rel="noopener">cuDNN下载地址</a>，我下的7.0.5版本<br><img src="https://img-blog.csdnimg.cn/20190519160003336.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzMyNjczNDUz,size_1,color_FFFFFF,t_70" alt="cuDNN下载"></p><h3 id="2、安装驱动"><a href="#2、安装驱动" class="headerlink" title="2、安装驱动"></a>2、安装驱动</h3><h4 id="2-1、正常装驱动。"><a href="#2-1、正常装驱动。" class="headerlink" title="2.1、正常装驱动。"></a>2.1、正常装驱动。</h4><p>按ctrl+alt+f2（有的是f1）进入字符界面命令行，先删除以前的驱动：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">sudo apt-get purge nvidia*</span><br><span class="line">sudo apt-get autoremove</span><br></pre></td></tr></table></figure><p>禁止自带的nouveau nvidia驱动：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># 打开配置文件</span><br><span class="line">sudo vim /etc/modprobe.d/blacklist-nouveau.conf</span><br></pre></td></tr></table></figure><p>添加以下内容：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">blacklist nouveau</span><br><span class="line">options nouveau modeset=0</span><br></pre></td></tr></table></figure><p>再更新一下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo update-initramfs -u</span><br></pre></td></tr></table></figure><p>最后需要进行重启。查看下Nouveau是否已经禁止，无输出则为成功：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">lsmod | grep nouveau</span><br></pre></td></tr></table></figure><p>按ctrl+alt+f2，接着关闭图形化界面：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo service lightdm stop</span><br></pre></td></tr></table></figure><p>然后准备开始装驱动了。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo sh NVIDIA-Linux-x86_64-XXX.run  –-no-opengl-files</span><br></pre></td></tr></table></figure><p>然后重新打开图形界面：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo service lightdm start</span><br></pre></td></tr></table></figure><p>再ctrl+alt+f7进入图形界面，再测试下驱动是否装好：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">nvidia-smi</span><br></pre></td></tr></table></figure><p>安装完成后，重启:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo reboot</span><br></pre></td></tr></table></figure><p>在命令行通过nvidia-smi还可以查看到驱动的话就没有问题了，以上皆为顺利的过程。</p></li></ol><h4 id="2-2、意外情况"><a href="#2-2、意外情况" class="headerlink" title="2.2、意外情况"></a>2.2、意外情况</h4><p>当然我装的时候是遇到了个大坑的。我看到之前机器上装好了驱动就没管，然后开始装后面的CUDA，结果下的CUDA又是deb的包，导致安装中覆盖了之前的驱动，然后ubuntu打开正确输入密码也无法进入桌面了。</p><h5 id="2-2-1-安装libelf-dev"><a href="#2-2-1-安装libelf-dev" class="headerlink" title="2.2.1 安装libelf-dev"></a>2.2.1 安装libelf-dev</h5><p>于是我又修复，倒回到2.1开始，清理驱动，重装。中间在执行sudo sh NVIDIA-Linux-x86_64-XXX.run  –-no-opengl-files的时候还遇到了build出错，如图：<br><img src="https://img-blog.csdnimg.cn/20190519162933827.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzMyNjczNDUz,size_1,color_FFFFFF,t_70" alt="驱动编译出错"><br>打开他提示的nvidia-installer.log看，里面提示了很多<br><img src="https://img-blog.csdnimg.cn/20190519163249938.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzMyNjczNDUz,size_1,color_FFFFFF,t_70" alt="问题提示"><br>这里还挺好的提示了请安装libelf-dev这种信息，于是我又去下载 <a href="https://pkgs.org/download/libelf-dev" target="_blank" rel="noopener">libelf-dex安装包</a>。本来我只下了1那个，然后输入命令安装：<br><img src="https://img-blog.csdnimg.cn/20190519163856594.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzMyNjczNDUz,size_1,color_FFFFFF,t_70" alt="libelf的版本"><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo dpkg -i libelf-dev_0.165-3ubuntu1_amd64.deb</span><br></pre></td></tr></table></figure><br>很无情的又报了个错，提示amd64 system is ….ubuntu1.1，于是我又下了2那个更新包，再dpkg安装。<br><img src="https://img-blog.csdnimg.cn/20190519164151594.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzMyNjczNDUz,size_1,color_FFFFFF,t_70" alt="在这里插入图片描述"><br>终于顺利给装上了，没有报错了。</p><h5 id="2-2-2-gcc和g-版本问题"><a href="#2-2-2-gcc和g-版本问题" class="headerlink" title="2.2.2 gcc和g++版本问题"></a>2.2.2 gcc和g++版本问题</h5><p>前面的装好了，我又准备执行sudo sh NVIDIA-Linux-x86_64-XXX.run  –-no-opengl-files 来着，然而还有问题，又通过命令查看log信息，sudo vim nvidia-installer.log。<br><img src="https://img-blog.csdnimg.cn/20190519164725139.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzMyNjczNDUz,size_1,color_FFFFFF,t_70" alt="不识别Command line"><br>这个问题就是由于gcc和g++版本太低编译不过导致的，因为我看之前有个教程是将这个版本降低了方便CUDA编译来着。但其实我这是CUDA9.0，CUDA9要求GCC版本是5.x或者6.x，其他版本不可以，需要自己进行配置。我之前就是5.5的版本，就不该降级。好的现在再根据那篇博文给换回来。<br><a href="https://blog.csdn.net/u011784994/article/details/80080938" target="_blank" rel="noopener">ubuntu 16.04 LTS 降级安装gcc 4.8</a></p><h5 id="2-2-3-装好驱动"><a href="#2-2-3-装好驱动" class="headerlink" title="2.2.3 装好驱动"></a>2.2.3 装好驱动</h5><p>在sh NVIDIA-Linux-x86_64-XXX.run安装就可以了，哎哟喂真是不容易啊。。。<br>然后我再重启，输入密码，终于可以进入桌面了呀，感动到哭。。。</p><h3 id="3、安装CUDA"><a href="#3、安装CUDA" class="headerlink" title="3、安装CUDA"></a>3、安装CUDA</h3><ol><li>安装CUDA<br>打开终端，执行命令，运行run文件：<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo sh cuda_9.0.176_384.81_linux.run</span><br></pre></td></tr></table></figure>注意提示，前面是一些法律信息啥的，enter过去就好。到后面提示是否安装图像驱动的时候，一定选择no ！！！<br><img src="https://img-blog.csdnimg.cn/20190519165519471.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzMyNjczNDUz,size_4,color_FFFFFF,t_70" alt="no Driver"><br>后面的一些提示选择y就行。出现下图，就表示安装完成。<br><img src="https://img-blog.csdnimg.cn/20190519165702507.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzMyNjczNDUz,size_1,color_FFFFFF,t_70" alt="CUDA安装"><br>如果出现其他问题，可能是某些依赖库没装好，反正我是没遇到。可以试试安装依赖，然后重启再试试。<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">sudo apt-get install libprotobuf-dev libleveldb-dev libsnappy-dev libopencv-dev libhdf5-serial-dev protobuf-compiler</span><br><span class="line">sudo apt-get install --no-install-recommends libboost-all-dev</span><br><span class="line">sudo apt-get install libopenblas-dev liblapack-dev libatlas-base-dev</span><br><span class="line">sudo apt-get install libgflags-dev libgoogle-glog-dev liblmdb-dev</span><br></pre></td></tr></table></figure></li></ol><ol><li><p>配置环境变量</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo gedit ~/.bashrc</span><br></pre></td></tr></table></figure><p>打开文件后在最后写入：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">export PATH=/usr/local/cuda-9.0/bin$&#123;PATH:+:$&#123;PATH&#125;&#125;  </span><br><span class="line">export LD_LIBRARY_PATH=/usr/local/cuda-9.0/lib64$&#123;LD_LIBRARY_PATH:+:$&#123;LD_LIBRARY_PATH&#125;&#125;</span><br></pre></td></tr></table></figure><p>然后点save后关闭在source一下生效：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">source ~/.bashrc</span><br></pre></td></tr></table></figure></li><li><p>测试一下CUDA是否安装成功</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"># 第一步，进入例子文件</span><br><span class="line">cd /usr/local/cuda-8.0/samples/1_Utilities/deviceQuery</span><br><span class="line"># 第二步，执行make命令</span><br><span class="line">sudo make</span><br><span class="line"># 第三步</span><br><span class="line">./deviceQuery</span><br></pre></td></tr></table></figure><p>有提示GPU信息，就表示可以了。</p></li></ol><h3 id="4、安装cuDNN"><a href="#4、安装cuDNN" class="headerlink" title="4、安装cuDNN"></a>4、安装cuDNN</h3><p>安装命令：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">sudo dpkg -i libcudnn7_7.0.5.15-1+cuda9.0_amd64.deb</span><br><span class="line">sudo dpkg -i libcudnn7-dev_7.0.5.11-1+cuda9.0_amd64.deb</span><br><span class="line">sudo dpkg -i libcudnn7-doc_7.0.5.11-1+cuda9.0_amd64.deb</span><br></pre></td></tr></table></figure><br>安装完以后需要进行测试是否安装成功，出现了“Test passed! ”，这几步我都没啥问题：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">cp -r /usr/src/cudnn_samples_v7/ $HOME</span><br><span class="line">cd $HOME/cudnn_samples_v7/mnistCUDNN</span><br><span class="line">make clean &amp;&amp; make</span><br><span class="line">./mnistCUDNN</span><br></pre></td></tr></table></figure></p><h3 id="5、安装TensorFlow-gpu"><a href="#5、安装TensorFlow-gpu" class="headerlink" title="5、安装TensorFlow-gpu"></a>5、安装TensorFlow-gpu</h3><p>卸载以前的TensorFlow，我的python环境是3.6<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip3 uninstall tensorflow</span><br></pre></td></tr></table></figure><br>然后重新装gpu版本就可以，注意我要用的是TensorFlow-gpu1.7版本，这个跟前面的都是对应的！</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip3 -i https://pypi.tuna.tsinghua.edu.cn/simple/ install tensorflow-gpu==1.7.0</span><br></pre></td></tr></table></figure><p>跑程序的时候，自动就调用了gpu进行计算，学习起来快了6、7倍，真的是开心啊~</p><h3 id="6、总结"><a href="#6、总结" class="headerlink" title="6、总结"></a>6、总结</h3><ol><li>最关键的问题就是软件各个版本要对应好</li><li>注意先装驱动再CUDA再cuDNN，总之就是驱动要先搞好，不然就会有我那种意外。</li><li>CUDA一定下载runfile文件。</li></ol><h3 id="7、reference"><a href="#7、reference" class="headerlink" title="7、reference"></a>7、reference</h3><p><a href="https://blog.csdn.net/weixin_41863685/article/details/80303963" target="_blank" rel="noopener">Ubuntu18.04深度学习GPU环境配置</a><br>我进不了桌面，也连不了网，所以都是自己拿另外的电脑下了U盘弄过去的。<br><a href="https://blog.csdn.net/hhhhh89/article/details/54311161" target="_blank" rel="noopener">ubuntu中使用终端查看U盘里的内容</a><br><a href="https://blog.csdn.net/u011784994/article/details/80080938" target="_blank" rel="noopener">ubuntu 16.04 LTS 降级安装gcc 4.8</a><br><a href="https://blog.csdn.net/omodao1/article/details/83241074" target="_blank" rel="noopener">Tensorflow不同版本要求与CUDA及CUDNN版本对应关系</a><br>最后感谢各个外援~</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h3 id=&quot;1、准备&quot;&gt;&lt;a href=&quot;#1、准备&quot; class=&quot;headerlink&quot; title=&quot;1、准备&quot;&gt;&lt;/a&gt;1、准备&lt;/h3&gt;&lt;ol&gt;
&lt;li&gt;请先看好各种软件的版本对应要求，这仨一定要对应好。&lt;pre&gt;&lt;code&gt;  [Tensorflow不同版本要求与
      
    
    </summary>
    
    
      <category term="深度学习" scheme="http://yoursite.com/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="深度学习" scheme="http://yoursite.com/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="配置" scheme="http://yoursite.com/tags/%E9%85%8D%E7%BD%AE/"/>
    
  </entry>
  
  <entry>
    <title>20190707《明朝那些事1》明朝的建立</title>
    <link href="http://yoursite.com/2019/07/07/20190707%E3%80%8A%E6%98%8E%E6%9C%9D%E9%82%A3%E4%BA%9B%E4%BA%8B1%E3%80%8B%E6%98%8E%E6%9C%9D%E7%9A%84%E5%BB%BA%E7%AB%8B/"/>
    <id>http://yoursite.com/2019/07/07/20190707%E3%80%8A%E6%98%8E%E6%9C%9D%E9%82%A3%E4%BA%9B%E4%BA%8B1%E3%80%8B%E6%98%8E%E6%9C%9D%E7%9A%84%E5%BB%BA%E7%AB%8B/</id>
    <published>2019-07-07T04:01:07.000Z</published>
    <updated>2019-12-10T04:06:01.922Z</updated>
    
    <content type="html"><![CDATA[<h3 id="一、序言"><a href="#一、序言" class="headerlink" title="一、序言"></a>一、序言</h3><p>以前我小觑了明朝，看完此书方知其宏伟恢弘。一个持久了两三百年的王朝，中间既有繁荣、胜利、正气；亦有凋敝、惨败与阴邪。历史有趣的就是在这来来回回的博弈中，道义精神的永不磨灭，历史规律的永恒不变。在此，我存着对历史的温情与敬意，以人物性格的角度，记录二三。</p><h3 id="二、人物"><a href="#二、人物" class="headerlink" title="二、人物"></a>二、人物</h3><p>明太祖朱元璋，最初本是穷苦人家的放牛娃。为生计所迫曾辗转为和尚，后来饥荒和压迫，最终他连和尚也做不成了，云游了几年加入了红巾军。在农民军里，他是一个很突出的人，不但作战勇敢，而且很有计谋，处事冷静，思虑深远，还很讲义气，有危险的时候第一个上，这一切都让他有了崇高的威信。</p><p><strong>将军——统率之人，必有更多素质要求。其中战略、远见、理想、勇气、气量等等皆不可缺。</strong></p><p>后续，朱元璋大败陈友谅、消灭张士诚，这些都是很精彩的战役。他不仅个人强，周边的人也都很强。他有贤内助妻子马皇后；身边大将如云、徐达、常遇春、李文忠、冯胜、朱文正、耿炳文、参谋刘基、李善长等等，我仅选部分介绍，详细的还是看书吧。</p><p>陈友谅，敢作敢当，但心黑手狠，胆大妄为，不重义气、背信弃义、骄横暴力。最终被诱敌深入的伏击给干掉了。巧的是那场鄱阳湖决战真的很像赤壁之战，果真历史来回重现。</p><p>张士诚，有勇气、意志坚强、却无大志，但他的的确确是个大好人。他待人宽大，免除了江浙一带的赋税。但他的过于宽大和无主见也使得他无法成为枭雄，而只能做一个豪杰。乱世中小富即安的思想可是不够生存的，在这种历史的淘汰赛里，只有胜负。</p><p>此处引用下朱元璋的战略分析，果真知人知彼啊，所以最后的赢家是朱元璋。</p><blockquote><p>张士诚的特点是器小，陈友谅的特点是志骄；器小无远见，志骄好生事。如果我进攻陈友谅，张士诚必然不会救他；而进攻张士诚，陈友谅就一定会动员全国兵力来救，我就要两线作战，到时就很难说了。</p></blockquote><p>马皇后，一心一意对待朱元璋，贤良仁德。在朱元璋称帝后乱杀大臣，马皇后“刀下留人”救了众多开国功臣。在教育子女上，也是要求他们生活简朴、用功读书。</p><p><strong>这样的女子不知道为朱元璋笼络了多少人心、培养了多少子女人才啊。</strong></p><p>常遇春，先锋大将，冷静观察形势，勇猛敢站，擅长骑兵突破，但却嗜好杀戮。后来常遇春主动向陈友谅挑事，活埋了降兵三千，带来了很多麻烦。</p><p><strong>可见，一个人的缺陷会很有可能导致大问题出现。</strong></p><p>徐达、善谋略、身先士卒、令出无二、为人谨慎，刚毅武勇，持重有谋，纪律严明，屡统大军，转战南北，治军严整，功高不矜，名列功臣第一。他是大破元军的关键人物，他也是活到最后的人之一了。</p><p>朱文正，善防守、排兵布阵。有军事才能，却不懂为人，性格乖张，心胸狭隘，最后竟然因为分攻奖赏不满而勾结张士诚，最终被囚禁。</p><p>刘基，神机军事，年少好学，运筹帷幄，准确判断。陈友谅进攻，其他人都在建议撤退之时，只有他在坚持，并且提出了诱敌伏击的策略。在多次战役中，他的判断甚至比朱元璋的判断还要准确。“三分天下诸葛亮，一统江山刘伯温”，在我看来他甚至比诸葛亮的成就还要高呢。不过，可惜最终死于政治斗争中。</p><p><strong>学习和实践从来都是成为一个有所建树的人的前提条件。</strong></p><h3 id="三、总结"><a href="#三、总结" class="headerlink" title="三、总结"></a>三、总结</h3><p>最后引用下原文对优秀将领成长过程的总结：</p><blockquote><p>第一个年级要学习的是军事理论。所有想成为名将的人，必须要学习一些经典的理论知识，包括《孙子兵法》《吴子兵法》等等。<br>第二个年级学习的内容是实战。这是极为重要的，那些理论学习的优秀者如果不能过这一关，他们就将被授予一个光荣的称号——纸上谈兵。<br>三年级要学习的是冷酷。 成为一个名将，就必须和仁慈、温和之类的名词说再见。他必须心如铁石、冷酷无情。<br>四年级要学习的是理智。<br>五年级学习判断，准确判断并决策。<br>六年级学习坚强，那些最优秀的人能够从失败中爬起来，去挑战那个多次战胜自己的人，这就叫做坚强。</p></blockquote><p>明朝的建立，经历了好几场大战。战场千变万化，胜者的智慧，败者的教训都是值得学习借鉴的。毕竟从人性、历史规律上看，一切都还是有章可循的。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h3 id=&quot;一、序言&quot;&gt;&lt;a href=&quot;#一、序言&quot; class=&quot;headerlink&quot; title=&quot;一、序言&quot;&gt;&lt;/a&gt;一、序言&lt;/h3&gt;&lt;p&gt;以前我小觑了明朝，看完此书方知其宏伟恢弘。一个持久了两三百年的王朝，中间既有繁荣、胜利、正气；亦有凋敝、惨败与阴邪。历史有趣
      
    
    </summary>
    
    
      <category term="读书笔记" scheme="http://yoursite.com/categories/%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="读书笔记" scheme="http://yoursite.com/tags/%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/"/>
    
      <category term="历史" scheme="http://yoursite.com/tags/%E5%8E%86%E5%8F%B2/"/>
    
  </entry>
  
  <entry>
    <title>20190507paper the Numenta Anomaly Benchmark</title>
    <link href="http://yoursite.com/2019/05/07/20190507paper-the-Numenta-Anomaly-Benchmark/"/>
    <id>http://yoursite.com/2019/05/07/20190507paper-the-Numenta-Anomaly-Benchmark/</id>
    <published>2019-05-07T13:38:23.000Z</published>
    <updated>2019-12-11T09:31:32.856Z</updated>
    
    <content type="html"><![CDATA[<h3 id="1-ABS-amp-Introduction"><a href="#1-ABS-amp-Introduction" class="headerlink" title="1 ABS &amp; Introduction"></a>1 ABS &amp; Introduction</h3><h4 id="1-1-Abstract"><a href="#1-1-Abstract" class="headerlink" title="1.1 Abstract"></a>1.1 Abstract</h4><p>对象：streams, time-series data, sequence</p><p>异常检测难点：real-time processing</p><p>NAB是一个测试评估针对流数据的异常检测算法的开源工具。</p><p>理想的异常检测器 </p><ol><li><p>检测到所有出现的异常 </p></li><li><p>尽早检测出异常，最好在人们看到异常之前 </p></li><li><p>no FP 不误报 </p></li><li><p>实时检测、没有前瞻（不看前面的数据） </p></li><li><p>自动化检测、无人工调节</p></li><li>适用性广泛，具有泛化性</li></ol><h4 id="1-2-Intro"><a href="#1-2-Intro" class="headerlink" title="1.2 Intro"></a>1.2 Intro</h4><p>静态基准不适合用于实时性算法。Precision和Recall无法反应出早检测这个效果。人工划分训练集测试集不适合流场景。NAB设计了新的评价标准，整合了各类数据集。其他数据集还有the UC- Irvine dataset ，Yahoo Labs。本论文比较了HTM、Skyline、与Twitter的两种方法<a href="https://blog.twitter.com/engineering/en_us/a/2015/introducing-practical-and-robust-anomaly-detection-in-a-time-series.html" target="_blank" rel="noopener">Twitter 方法，翻</a> （AnomalyDetectionTs and AnomalyDetectionVec. ）。</p><h3 id="2，NAB-scoring"><a href="#2，NAB-scoring" class="headerlink" title="2，NAB scoring"></a>2，NAB scoring</h3><h4 id="2-1-基础"><a href="#2-1-基础" class="headerlink" title="2.1 基础"></a>2.1 基础</h4><p>异常定义：We define anomalies in a data stream to be patterns that do not conform to past patterns of behavior for the stream. 包括空间异常和时间异常。</p><p><img src="/images/20191210anomaly.jpg" alt="20191210anomaly"></p><p>Dataset：范围从IT指标（例如网络利用率）到工业机器上的传感器再到社交媒体聊天。我们还包括一些人工生成的数据文件，用于测试尚未在语料库的真实数据中表示的异常行为，以及几个没有任何异常的数据文件。当前的NAB数据集包含58个数据文件，每个文件具有1000-22,000个数据实例（github里有）</p><p>标记异常：按一定规则，标记ground truth label</p><h4 id="2-2-算法"><a href="#2-2-算法" class="headerlink" title="2.2 算法"></a>2.2 算法</h4><p>算法核心三个方面：anomaly Window，the scoring function，application Profiles（配置文件）</p><h5 id="2-2-1-异常窗口"><a href="#2-2-1-异常窗口" class="headerlink" title="2.2.1 异常窗口"></a>2.2.1 异常窗口</h5><p>异常窗口是代表一系列以真实异常标签（ a ground truth anomaly label ）为中心的数据点。</p><p>异常窗口的作用是判断真假检测，检测在窗外的话是FP。</p><p>评分函数基于窗口识别、加权TP，FP，FN。前面紫色部分只用来初始学习，不需测试。</p><p>1，窗口内最早的TP检测被计分，其他忽略。</p><p>2，sigmoidal scoring function 给早检测的TP高分。给FP负分数。</p><p>3，窗口大小 = 10%*总数据长度/异常数量。实验测试了5% - 20%，由于缩放评分函数，这个百分比对最后结果不敏感。</p><p><img src="/images/20191210AnomalyWindow.jpg" alt="20191210AnomalyWindow"></p><p>application profile配置：FN对工业机器来说会造成损失，FP要求技术人员查看。（对监视数据中心中各个服务器状态的应用程序可能对误报的数量敏感，并且由于大多数服务器群集都相对容错，因此偶尔会遗漏异常情况很好。）因此配置文件用于：对于TP，FP，FN和TN，NAB应用与每个配置文件相关的不同相对权重以获得每个配置文件的单独分数。</p><h5 id="2-2-2-计算过程"><a href="#2-2-2-计算过程" class="headerlink" title="2.2.2 计算过程"></a>2.2.2 计算过程</h5><p>1，配置权重$A$</p><script type="math/tex; mode=display">A_{T P}, A_{F P}, A_{F N}, A_{T N}, 0 \leq A_{TP},A_{TN} \leq 1, -1 \leq A_{FP},A_{FN} \leq 0</script><p>$D$ 是数据集，$Y_d$是数据 $d$ 中被检测出来的异常。$f_d$表示没有检测到任何异常的窗口数量，</p><p><img src="/images/20191210NABScoring.jpg" alt="20191210NABScoring"></p><p>2，单个窗口的得分计算</p><p>图中TP：早检测则，增加NAB score；点2：早检测的TP，贡献+0.999 。</p><p>FP：减分（窗外后面的FP的减分更大）；点1：FP，贡献-1，点4：权重为-0.8093是根据$\sigma^{A}(y)$得到的。5：5更有害，因此5贡献-1。</p><p>FN：完全没有检测到，减分。</p><p>总的来看这个窗口的得分就是：$−1.0A_{FP} + 0.9999A_{TP} −0.8093A_{FP} − 1.0A_{FP} $ ，公式是：</p><script type="math/tex; mode=display">\sigma^{A}(y)=\left(A_{T P}-A_{F P}\right)\left(\frac{1}{1+e^{5 y}}\right)-1</script><p>$\sigma^{A}(y)$中，y表示是检测在异常检测窗的相对位置，参数被设置为窗口右侧，$\sigma ( y = 0.0 ) = 0$。 </p><p>3，一个数据文件的得分计算</p><p>得分是每个检测的得分+错过的Window</p><script type="math/tex; mode=display">S_{d}^{A}=\left(\sum_{y \in Y_{d}} \sigma^{A}(y)\right)+A_{F N} f_{d}</script><p>4，一个异常检测算法对所有数据集的得分</p><script type="math/tex; mode=display">S ^ { A } = \sum _ { d \in D } S _ { d } ^ { A }</script><p>5，归一化这个算法的分数，normalized NAB score</p><script type="math/tex; mode=display">S _ { N A B } ^ { A } = 100 \cdot \frac { S ^ { A } - S _ { \text {null} } ^ { A } } { S _ { \text {perfect} } ^ { A } - S _ { \text {null} } ^ { A } }</script><p>完美检测器检测到所有TP，无FP。NULL检测器就是没有检测到任何异常。</p><h5 id="2-2-3-其他"><a href="#2-2-3-其他" class="headerlink" title="2.2.3 其他"></a>2.2.3 其他</h5><p>HTM算法，Skyline统计算法，Twitter统计算法等</p><p>每个异常检测器输出是0-1之间的分数，使用固定阈值对分数进行阈值处理，以检测异常。 NAB包括自动爬坡搜索，用于为每种算法选择最佳阈值。其中要最大化的目标函数是NAB评分函数。一个阈值针对所有的数据集dataset（The detection threshold is thus tuned based on the full NAB dataset）。</p><h3 id="3-result"><a href="#3-result" class="headerlink" title="3 result"></a>3 result</h3><p>见github首页</p><p>一些小结论</p><p>1，HTM和Skyline对漂移适应得更快</p><p>2，HTM和Skyline各自也有误报但HTM可以早检测（3h，机器温度传感器数据）</p><p>3，行为的时间变化通常先于容易检测到的大变化（做提前检测）。 </p><h3 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h3><p>1，Lavin A, Ahmad S. Evaluating Real-Time Anomaly Detection Algorithms—The Numenta Anomaly Benchmark[C]//2015 IEEE 14th International Conference on Machine Learning and Applications (ICMLA). IEEE, 2015: 38-44.</p><p>2, <a href="https://github.com/numenta/NAB" target="_blank" rel="noopener">https://github.com/numenta/NAB</a> 很好的学习项目</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h3 id=&quot;1-ABS-amp-Introduction&quot;&gt;&lt;a href=&quot;#1-ABS-amp-Introduction&quot; class=&quot;headerlink&quot; title=&quot;1 ABS &amp;amp; Introduction&quot;&gt;&lt;/a&gt;1 ABS &amp;amp; Introd
      
    
    </summary>
    
    
      <category term="AIOps" scheme="http://yoursite.com/categories/AIOps/"/>
    
    
      <category term="AIOps" scheme="http://yoursite.com/tags/AIOps/"/>
    
      <category term="时间序列" scheme="http://yoursite.com/tags/%E6%97%B6%E9%97%B4%E5%BA%8F%E5%88%97/"/>
    
  </entry>
  
  <entry>
    <title>装window、ubuntu双系统</title>
    <link href="http://yoursite.com/2019/04/24/%E8%A3%85window%E3%80%81ubuntu%E5%8F%8C%E7%B3%BB%E7%BB%9F/"/>
    <id>http://yoursite.com/2019/04/24/%E8%A3%85window%E3%80%81ubuntu%E5%8F%8C%E7%B3%BB%E7%BB%9F/</id>
    <published>2019-04-24T01:09:12.000Z</published>
    <updated>2019-11-06T03:07:09.579Z</updated>
    
    <content type="html"><![CDATA[<h2 id="装window10、ubuntu16-04双系统"><a href="#装window10、ubuntu16-04双系统" class="headerlink" title="装window10、ubuntu16.04双系统"></a>装window10、ubuntu16.04双系统</h2><p>周末趁空装了个双系统，记录记录过程吧。</p><h3 id="装windows10"><a href="#装windows10" class="headerlink" title="装windows10"></a>装windows10</h3><ol><li>首先下载好win10的系统镜像ISO文件，由于我不咋用win10就装了家庭版<br>链接: <a href="http://pan.baidu.com/s/1sj3JNRJ" target="_blank" rel="noopener">http://pan.baidu.com/s/1sj3JNRJ</a> 密码: z49r</li></ol><ol><li><p>准备好空的U盘，准备做系统启动盘。<br>下载安装好UltraISO，插入U盘。<br>点击打开，选择ISO文件<br>点击启动 - 写入硬盘映像<br>写入方式选择的是USB-HDD，USB-HDD+，一般默认就好<br>在点击写入，就等着他默默写好就好了<br><img src="https://img-blog.csdnimg.cn/20190422141325858.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzMyNjczNDUz,size_16,color_FFFFFF,t_70" alt="ULtraISO刻录系统启动盘"></p></li><li><p>制作好的系统启动U盘插入要装系统的电脑。开启电脑，一直按 F2或在F12等（这个键根据电脑确定，可以查查，但一般就是这个），进入电脑的Bios设置。<br>选择usb storage device，放到最前面，表示系统启动优先从USB开始。点击apply，再点exit。</p></li><li><p>之后电脑自动重启，然后进入windows10的安装。<br>默认简体中文，下一步<br>哪种类型的安装：选择自定义，以前windows的东西会变成windows.old<br>输入产品密钥那里跳过。<br>你想将windows安装在哪？ 选择分区，选择之前C盘所在分区位置。我这选择的是分区1，476G的盘。<br>后面就等着自己装就好了。</p></li><li><p>装完后注意，系统会重新启动。此时要拔掉U盘。产品密钥那个后面可以去找破解工具破解。暂时不管，然后设置用户密码进入就好。</p></li></ol><h3 id="装ubuntu16-04"><a href="#装ubuntu16-04" class="headerlink" title="装ubuntu16.04"></a>装ubuntu16.04</h3><ol><li>同理下载好U盘，将ubuntu的系统镜像刻录到U盘里。</li><li>设置好bios优先从U盘启动。</li><li>preparing to install Ubuntu: 这里可以选择第二项（Erase disk and install Ubuntu 单独装个Ubuntu系统）或者something else（我这装双系统，本来电脑里分区比较多，因此要选择之前从window划分出来的空闲空间）</li><li>挂载分区到根路径 / ,如果空间足够大，就只挂载这个，剩下的Ubuntu自己会分。如果不够，可以单独跟/home , /boot那些单独分。<br><img src="https://img-blog.csdnimg.cn/2019042409082143.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzMyNjczNDUz,size_16,color_FFFFFF,t_70" alt="挂载"></li><li>继续时区，创建用户，后面就会重启了。重启的时候，注意拔掉U盘。</li></ol>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;装window10、ubuntu16-04双系统&quot;&gt;&lt;a href=&quot;#装window10、ubuntu16-04双系统&quot; class=&quot;headerlink&quot; title=&quot;装window10、ubuntu16.04双系统&quot;&gt;&lt;/a&gt;装window10、ubun
      
    
    </summary>
    
    
      <category term="配置" scheme="http://yoursite.com/categories/%E9%85%8D%E7%BD%AE/"/>
    
    
      <category term="配置" scheme="http://yoursite.com/tags/%E9%85%8D%E7%BD%AE/"/>
    
      <category term="装系统" scheme="http://yoursite.com/tags/%E8%A3%85%E7%B3%BB%E7%BB%9F/"/>
    
      <category term="ubuntu" scheme="http://yoursite.com/tags/ubuntu/"/>
    
  </entry>
  
  <entry>
    <title>20190408AnomalyDetectionBackground</title>
    <link href="http://yoursite.com/2019/04/08/20190408AnomalyDetectionBackground/"/>
    <id>http://yoursite.com/2019/04/08/20190408AnomalyDetectionBackground/</id>
    <published>2019-04-08T04:35:30.000Z</published>
    <updated>2019-12-12T04:37:18.785Z</updated>
    
    <content type="html"><![CDATA[<h3 id="1-背景"><a href="#1-背景" class="headerlink" title="1 背景"></a>1 背景</h3><h4 id="1-1-企业背景"><a href="#1-1-企业背景" class="headerlink" title="1.1 企业背景"></a>1.1 企业背景</h4><p>分布式系统结构的广泛应用。具有高并发，低时延，高可靠性等特点，但同时由于需求的增长，其规模，复杂性和动态生成的数据也急剧增加，这使其可靠性降低。为了避免系统故障，因此异常检测故障预判很重要。</p><p>简单来说目前的一些应用痛点，也是我企业调研的结果：</p><p>1，测试人员时间有限，不能有效测试，全覆盖测试。系统BUG是难免的。</p><p>2，系统故障后排查困难，需要及时定位。</p><p>3，运维人员希望可以提前预测故障，越早越好，从而进行排查。</p><p>4，目前企业的监控数据是有的，如何利用起来对系统更好的运维。</p><h4 id="2-1-研究背景"><a href="#2-1-研究背景" class="headerlink" title="2.1 研究背景"></a>2.1 研究背景</h4><p>流数据的异常检测难点有：</p><p>1，流数据高速实时产生  ，传统的对整个数据集离线学习很难。</p><p>2，异常行为很少发生，异常检测器训练困难，难以学习对于重要的不平衡数据集的满意模型。</p><p>3，流数据的时变特性。两类异常，空间异常和上下文异常。概念漂移问题。</p><p>4，Precision与Recall之前的权衡问题。</p><p>5，不同的时序数据有不同属性。周期性，平稳性，非平稳性等等性质，对不同的方法有要求。</p><p>6，异常数据的标记很难得。</p><p>7，提前检测很重要，也很困难。</p><h4 id="2-2-智能运维背景"><a href="#2-2-智能运维背景" class="headerlink" title="2.2 智能运维背景"></a>2.2 智能运维背景</h4><p>于是Gartner首先推出了人工智能运算（AIOP，这个方向国内还有清华大学的裴丹老师），包括性能监视，异常检测和系统故障检测任务等。理想的智能运维具有以下能力：历史数据管理、流数据（即时序数据）管理、日志数据提取、网络数据提取、性能数据提取、文本数据提取、自动化模型的发现和预测、异常检测、根因分析、按需交付等  </p><blockquote><p>AIOps is the application of artificial intelligence for IT operations. It is the future of ITOps, combining algorithmic and human intelligence to provide full visibility into the state and performance of the IT systems that businesses rely on.</p></blockquote><p>性能监控里包括了对系统的CPU、memory，storage，网络，进程等资源使用的监控信息。通过对性能监控的时间序列进行异常检测，发现故障之前的征兆。</p><h3 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h3><p>1, “Everything you need to know about AIOps”, from <a href="https://www.moogsoft.com/resources/aiops/guide/everything-aiops/" target="_blank" rel="noopener">https://www.moogsoft.com/resources/aiops/guide/everything-aiops/</a> (retrieved as of Feb. 12, 2019)</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h3 id=&quot;1-背景&quot;&gt;&lt;a href=&quot;#1-背景&quot; class=&quot;headerlink&quot; title=&quot;1 背景&quot;&gt;&lt;/a&gt;1 背景&lt;/h3&gt;&lt;h4 id=&quot;1-1-企业背景&quot;&gt;&lt;a href=&quot;#1-1-企业背景&quot; class=&quot;headerlink&quot; title=&quot;1
      
    
    </summary>
    
    
      <category term="AIOps" scheme="http://yoursite.com/categories/AIOps/"/>
    
    
      <category term="AIOps" scheme="http://yoursite.com/tags/AIOps/"/>
    
      <category term="异常检测" scheme="http://yoursite.com/tags/%E5%BC%82%E5%B8%B8%E6%A3%80%E6%B5%8B/"/>
    
  </entry>
  
  <entry>
    <title>集成学习</title>
    <link href="http://yoursite.com/2018/12/25/20191225%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0/"/>
    <id>http://yoursite.com/2018/12/25/20191225%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0/</id>
    <published>2018-12-25T03:23:38.000Z</published>
    <updated>2019-12-28T08:18:09.459Z</updated>
    
    <content type="html"><![CDATA[<h3 id="1-简介"><a href="#1-简介" class="headerlink" title="1 简介"></a>1 简介</h3><p>集成学习（Ensemble learning）通过组合几种模型来提高机器学习的效果。构建并结合多个学习器，个体学习器要“好而不同”，一定的准确性/多样性。</p><h3 id="2-提升方法"><a href="#2-提升方法" class="headerlink" title="2 提升方法"></a>2 提升方法</h3><h4 id="2-1-提升方法之Adaboost"><a href="#2-1-提升方法之Adaboost" class="headerlink" title="2.1 提升方法之Adaboost"></a>2.1 提升方法之Adaboost</h4><p>一般过程：训练—基学习器—调整训练样本分布—重复得到更多基学习器 T个—将这T个基学习器加权结合。代表是Adaboost：提高那些被前一轮弱分类器分错的样本的权值。最后加权多数表决方法、加大分类误差率小的弱分类器的权值。属于序列集成。</p><p>算法（书P156）：</p><p>输入训练集$T=\left\{\left(x_{1}, y_{1}\right),\left(x_{2}, y_{2}\right), \cdots,\left(x_{N}, y_{N}\right)\right\}$，其中实例$x_{i} \in \mathcal{X}=\mathbf{R}^{n}$，$y_i \in \mathcal{Y} = \left\{ -1,+1 \right\}$</p><p>1，初始化训练数据的权值分布为均匀分布 。$w_{1i} = \frac{1}{N}$。</p><p>2，使用具有权值分布的$D_m$的训练数据集学习得到基分类器$G_{m}(x): \mathcal{X} \rightarrow\{-1,+1\}$。</p><p>3，计算$G_m(x)$在训练数据集上的分类误差率:</p><script type="math/tex; mode=display">e_{m}=\sum_{i=1}^{N} P\left(G_{m}\left(x_{i}\right) \neq y_{i}\right)=\sum_{i=1}^{N} w_{m i} I\left(G_{m}\left(x_{i}\right) \neq y_{i}\right)</script><p>计算得到$G_m(x)$的系数，它表示了$G_m(x)$在最终分类器中的重要性。他随$e_m$的减小而增大。</p><script type="math/tex; mode=display">\alpha_{m}=\frac{1}{2} \log \frac{1-e_{m}}{e_{m}}</script><p>在更新训练数据集的权值分布</p><script type="math/tex; mode=display">D_{m+1}=\left(w_{m+1,1}, \cdots, w_{m+1, i}, \cdots, w_{m+1, N}\right)</script><script type="math/tex; mode=display">w_{m+1, i}=\frac{w_{m i}}{Z_{m}} \exp \left(-\alpha_{m} y_{i} G_{m}\left(x_{i}\right)\right), \quad i=1,2, \cdots, N</script><p>其中$Z_m$是规范化因子：</p><script type="math/tex; mode=display">Z_{m}=\sum_{i=1}^{N} w_{m i} \exp \left(-\alpha_{m} y_{i} G_{m}\left(x_{i}\right)\right)</script><p>4，构建基分类器的线性组合 $f(x) = \sum_{m=1}^M  \alpha_m  G_m(x)$</p><p>得到最终分类器：</p><script type="math/tex; mode=display">G(x) = \operatorname{sign}\left(\sum_{m=1}^{M} \alpha_{m} G_{m}(x)\right)</script><h4 id="2-2-提升方法之提升树-Boosting"><a href="#2-2-提升方法之提升树-Boosting" class="headerlink" title="2.2 提升方法之提升树 Boosting"></a>2.2 提升方法之提升树 Boosting</h4><p>采用加法模型（基函数的线性组合，基函数为树的时候叫Boosting tree），前向分步算法。减小偏差。</p><h5 id="2-2-1-前向分步算法："><a href="#2-2-1-前向分步算法：" class="headerlink" title="2.2.1 前向分步算法："></a>2.2.1 前向分步算法：</h5><p>1，确定初始提升树$f_0(x) = 0$</p><p>2，第m步的模型是$f_m(x) = f_{m-1}(x) + T(x: \theta_m)$</p><p>这里需要通过经验风险极小化来确定下一棵决策树参数参数:</p><script type="math/tex; mode=display">\hat{\theta_m} = argmin_{\theta_m} \sum_{i=1}^N L(y_i, f_{m-1}(x_i) + T(x_i:\theta_m))</script><p>不同问题的提升树学习算法区别在于使用的损失函数不同。</p><div class="table-container"><table><thead><tr><th>问题</th><th>学习算法</th></tr></thead><tbody><tr><td>回归树</td><td>平方误差（拟合残差）</td></tr><tr><td>分类问题</td><td>指数损失函数</td></tr><tr><td>一般决策问题</td><td>一般损失函数</td></tr></tbody></table></div><p>对于二分类问题，提升树算法只需将Adaboost算法中基本分类器限制为二分类树即可。</p><h5 id="2-2-2-回归问题的提升树，拟合残差"><a href="#2-2-2-回归问题的提升树，拟合残差" class="headerlink" title="2.2.2 回归问题的提升树，拟合残差"></a>2.2.2 回归问题的提升树，拟合残差</h5><p>1，初始化$f0(x)=0$</p><p>2，对m=1,2..M计算残差，N是样本数。当前模型拟合数据的残差。</p><script type="math/tex; mode=display">r_{mi} = y_i -f_{m-1}(x_i), i=1.2..N</script><p>拟合残差$r_{mi}$学习一个回归树$T(x: \theta_m)$。更新：</p><script type="math/tex; mode=display">f_m(x) = f_{m-1}(xi) + T(x: \theta_m)</script><p>3，得到回归问题提升树 </p><script type="math/tex; mode=display">f_M(x) = \sum_{m=1}^M T(x; \theta_m)</script><h5 id="2-2-3-一般决策问题GBDT"><a href="#2-2-3-一般决策问题GBDT" class="headerlink" title="2.2.3 一般决策问题GBDT"></a>2.2.3 一般决策问题GBDT</h5><p>一般损失函数：</p><p>梯度提升gradientBoosting （GBDT）：利用最速下降法的近似方法，关键是利用损失函数的负梯度在当前模型的值</p><script type="math/tex; mode=display">-\left[\frac{\partial L\left(y, f\left(x_{i}\right)\right)}{\partial f\left(x_{i}\right)}\right]_{f(x)=f_{m-1}(x)}</script><p>作为回归问题提升树算法中的残差的近似值，拟合一个回归树。</p><p>过程：</p><p>输入训练集$T=\left\{\left(x_{1}, y_{1}\right),\left(x_{2}, y_{2}\right), \cdots,\left(x_{N}, y_{N}\right)\right\}$，其中实例$x_{i} \in \mathcal{X} \subseteq \mathbf{R}^{n}, y_{i} \in \mathcal{Y} \subseteq \mathbf{R}$</p><p>输出回归树： $\hat{f(x)}$</p><p>(1) 初始化</p><script type="math/tex; mode=display">f_{0}(x)=\arg \min _{c} \sum_{i=1}^{N} L\left(y_{i}, c\right)</script><p>(2) 对m = 1,2…M</p><p>对 i = 1,2,…N 计算：</p><script type="math/tex; mode=display">r_{m i}=-\left[\frac{\partial L\left(y_{i}, f\left(x_{i}\right)\right)}{\partial f\left(x_{i}\right)}\right]_{f(x)=f_{m-1}(x)}</script><p>对$r_{mi}$ 拟合一个回归树。得到第m棵树的叶节点区域$R_{m j}, j=1,2, \cdots, J$</p><p>对 j = 1,2 …J 计算：</p><script type="math/tex; mode=display">c_{m j}=\arg \min _{c} \sum_{x_{i} \in R_{m j}} L\left(y_{i}, f_{m-1}\left(x_{i}\right)+c\right)</script><p>更新</p><script type="math/tex; mode=display">f_{m}(x)=f_{m-1}(x)+\sum_{j=1}^{J} c_{m j} I\left(x \in R_{m j}\right)</script><p>(3) 得到回归树</p><script type="math/tex; mode=display">\hat{f}(x)=f_{M}(x)=\sum_{m=1}^{M} \sum_{j=1}^{J} c_{m j} I\left(x \in R_{m j}\right)</script><h4 id="2-3-其他"><a href="#2-3-其他" class="headerlink" title="2.3 其他"></a>2.3 其他</h4><p>XGBoost：</p><p>经过优化的分布式梯度提升（Gradient Boosting）库，实现了并行方式的决策树提升(Tree Boosting)。XGBoost采用的是level（depth）-wise生长策略，如下所示，能够同时分裂同一层的叶子，从而进行多线程优化，不容易过拟合；但不加区分的对待同一层的叶子，带来了很多没必要的开销。</p><p>XGBoost使用的是pre-sorted算法，能够更精确的找到数据分隔点；XGBoost在训练之前，预先对数据进行了排序，然后保存为block结构，后面的迭代中重复地使用这个结构，大大减小计算量。这个block结构也使得并行成为了可能，在进行节点的分裂时，需要计算每个特征的增益，最终选增益最大的那个特征去做分裂，那么各个特征的增益计算就可以开多线程进行。</p><p>XGBoost在代价函数里加入了正则项，用于控制模型的复杂度。正则项里包含了树的叶子节点个数、每个叶子节点上输出的score的L2模的平方和。</p><p><img src="/images/20191225Xgboost.jpg" alt="20191225Xgboost"></p><p>LightGBM：</p><p>LightGBM的设计思路主要是两点：1. 减小数据对内存的使用，保证单个机器在不牺牲速度的情况下，尽可能地用上更多的数据；2. 减小通信的代价，提升多机并行时的效率，实现在计算上的线性加速。</p><p>LightGBM采用leaf-wise生长策略，如Figure 2所示，每次从当前所有叶子中找到分裂增益最大（一般也是数据量最大）的一个叶子，然后分裂，如此循环；但会生长出比较深的决策树，产生过拟合。</p><p>LightGBM使用的是histogram算法，占用的内存更低，数据分隔的复杂度更低。</p><p><img src="/images/20191225LightGBM.jpg" alt="20191225LightGBM"></p><h3 id="3-Bagging"><a href="#3-Bagging" class="headerlink" title="3 Bagging"></a>3 Bagging</h3><h4 id="3-1-Bagging"><a href="#3-1-Bagging" class="headerlink" title="3.1 Bagging"></a>3.1 Bagging</h4><p>Bagging是有放回样本采样boostrap——产生互相有交叠的采样子集63.2% 。一般对分类任务使用简单投票法。剩下36.8%的数据可以用作验证集对泛化性能进行包外估计out-of-bag-estimate。</p><script type="math/tex; mode=display">f(x)=1 / M \sum_{m=1}^{M} f_{m}(x)</script><p>在不同样本集上训练不同的树，通常分类任务使用投票的方式集成，而回归任务通过平均的方式集成。减小方差。</p><h4 id="3-2-随机森林"><a href="#3-2-随机森林" class="headerlink" title="3.2 随机森林"></a>3.2 随机森林</h4><p>随机森林：样本采样+属性采样构建多棵决策树，最终决定结果。方差小，偏差也小。</p><h3 id="4-Stacking方法"><a href="#4-Stacking方法" class="headerlink" title="4 Stacking方法"></a>4 Stacking方法</h3><p>Stacking是通过一个元分类器或者元回归器来整合多个分类模型或回归模型的集成学习技术。基础模型利用整个训练集做训练，元模型将基础模型的特征作为特征进行训练。</p><p>其实就是先训练多个初级分类器，然后基于初级分类器对样本预测，将预测值作为新的训练集训练次级学习器。</p><p><img src="/images/20191225Stacking.jpg" alt="20191225Stacking"></p><h3 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h3><p>1，李航《统计学习方法》</p><p>2，<a href="https://zhuanlan.zhihu.com/p/36161812" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/36161812</a> 集成学习</p><p>3， <a href="https://blog.csdn.net/v_JULY_v/article/details/81410574" target="_blank" rel="noopener">https://blog.csdn.net/v_JULY_v/article/details/81410574</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h3 id=&quot;1-简介&quot;&gt;&lt;a href=&quot;#1-简介&quot; class=&quot;headerlink&quot; title=&quot;1 简介&quot;&gt;&lt;/a&gt;1 简介&lt;/h3&gt;&lt;p&gt;集成学习（Ensemble learning）通过组合几种模型来提高机器学习的效果。构建并结合多个学习器，个体学习器要“好而
      
    
    </summary>
    
    
      <category term="机器学习" scheme="http://yoursite.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="机器学习" scheme="http://yoursite.com/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>朴素贝叶斯</title>
    <link href="http://yoursite.com/2018/12/24/20191224%E6%9C%B4%E7%B4%A0%E8%B4%9D%E5%8F%B6%E6%96%AF/"/>
    <id>http://yoursite.com/2018/12/24/20191224%E6%9C%B4%E7%B4%A0%E8%B4%9D%E5%8F%B6%E6%96%AF/</id>
    <published>2018-12-24T12:36:12.000Z</published>
    <updated>2019-12-24T14:32:48.966Z</updated>
    
    <content type="html"><![CDATA[<h3 id="1-简介"><a href="#1-简介" class="headerlink" title="1 简介"></a>1 简介</h3><p>朴素贝叶斯法基于<u>贝叶斯定理</u>与<u>特征条件独立</u>假设的分类方法。对于给定的训练数据集，首先基于特征条件独立假设学习输入输出的联合概率分布（学习到生成数据的机制，是生成模型），然后基于此模型，对给定的输入x，利用贝叶斯定理求出后验概率最大的输出 $y$。</p><p>输入空间 $\mathcal{X} \subseteq \mathbf{R}^{n}$ 为n维向量的集合。</p><p>输出空间 $\mathcal{Y} = \left\{c_{1}, c_{2}, \cdots, c_{K}\right\}$</p><p>输入特征向量 $x$，输出类标记 $y$</p><p>随机向量$X$是定义在输入空间 $\mathcal{X}$，$Y$是定义在输出空间 $\mathcal{Y}$ 的随机变量。</p><p>训练数据集 $T=\left\{\left(x_{1}, y_{1}\right),\left(x_{2}, y_{2}\right), \cdots,\left(x_{N}, y_{N}\right)\right\}$ 由$P(X,Y)$ 独立同分布产生。</p><h4 id="1-1-学习"><a href="#1-1-学习" class="headerlink" title="1.1 学习"></a>1.1 学习</h4><p>朴素贝叶斯法先学习先验概率分布及条件概率分布。</p><p>先验概率分布：</p><script type="math/tex; mode=display">P(Y = c_k), k=1,2 \cdots K</script><p>条件概率分布：它有指数级数量的参数，基于条件独立性假设</p><script type="math/tex; mode=display">P\left(X=x | Y=c_{k}\right)=P\left(X^{(1)}=x^{(1)}, \cdots, X^{(n)}=x^{(n)} | Y=c_{k}\right), \quad k=1,2, \cdots, K</script><script type="math/tex; mode=display">=\prod_{j=1}^{n} P\left(X^{(j)}=x^{(j)} | Y=c_{k}\right)</script><p>条件独立假设：用于分类的特征在类确定的条件下，都是属于条件独立的</p><h4 id="1-2-预测"><a href="#1-2-预测" class="headerlink" title="1.2 预测"></a>1.2 预测</h4><p>对给定的输入$x$，通过学习到的模型计算后验概率，最大的类作为预测结果。</p><p>后验概率计算根据的是贝叶斯定理：</p><script type="math/tex; mode=display">P\left(Y=c_{k} | X=x\right)=\frac{P\left(X=x | Y=c_{k}\right) P\left(Y=c_{k}\right)}{\sum_{k} P\left(X=x | Y=c_{k}\right) P\left(Y=c_{k}\right)}</script><p>将前面学习到的代入得：</p><script type="math/tex; mode=display">P\left(Y=c_{k} | X=x\right)=\frac{P\left(Y=c_{k}\right) \prod_{j}^n P\left(X^{(j)}=x^{(j)} | Y=c_{k}\right)}{\sum_{k} P\left(Y=c_{k}\right) \prod_{j}^n P\left(X^{(j)}=x^{(j)} | Y=c_{k}\right)}, \quad k=1,2, \cdots, K</script><p>！！！因此<strong>朴素贝叶斯分类器</strong>就是这样子了：</p><script type="math/tex; mode=display">y = f(x)=argmin_{c_k} \frac{P\left(Y=c_{k}\right) \prod_{j}^n P\left(X^{(j)}=x^{(j)} | Y=c_{k}\right)}{\sum_{k} P\left(Y=c_{k}\right) \prod_{j}^n P\left(X^{(j)}=x^{(j)} | Y=c_{k}\right)}, \quad k=1,2, \cdots, K</script><h4 id="1-3-后验概率最大化的含义"><a href="#1-3-后验概率最大化的含义" class="headerlink" title="1.3 后验概率最大化的含义"></a>1.3 后验概率最大化的含义</h4><p>将实例分到后验概率最大的类中，等价于期望风险最小化。</p><p>假设损失函数：</p><script type="math/tex; mode=display">L(Y, f(X))=\left\{\begin{array}{ll}{1,} & {Y \neq f(X)} \\ {0,} & {Y=f(X)}\end{array}\right.</script><p>期望风险函数:</p><script type="math/tex; mode=display">R_{\mathrm{exp}}(f)=E[L(Y, f(X))]</script><p>取条件期望</p><script type="math/tex; mode=display">R_{\exp }(f)=E_{X} \sum_{k=1}^{K}\left[L\left(c_{k}, f(X)\right)\right] P\left(c_{k} | X\right)</script><p>为了使得期望风险最小化，需要对$X = x$ 逐个极小化:</p><script type="math/tex; mode=display">f(x)=\arg \min _{y \in \mathcal{Y}} \sum_{k=1}^{K} L\left(c_{k}, y\right) P\left(c_{k} | X=x\right)</script><script type="math/tex; mode=display">= \arg \min _{y \in \mathcal{Y}} \sum_{k=1}^{K} P\left(y \neq c_{k} | X=x\right)</script><script type="math/tex; mode=display">= \arg \max _{y \in \mathcal{Y}} P\left(y=c_{k} | X=x\right)</script><h3 id="2-极大似然估计"><a href="#2-极大似然估计" class="headerlink" title="2 极大似然估计"></a>2 极大似然估计</h3><p>学习即估计先验概率分布与条件概率分布：</p><script type="math/tex; mode=display">P\left(Y=c_{k}\right)=\frac{\sum_{i=1}^{N} I\left(y_{i}=c_{k}\right)}{N}, \quad k=1,2, \cdots, K</script><p>设第$j$个特征$x^{(j)}$的可能取值的集合为 $\left\{a_{j 1}, a_{j 2}, \cdots, a_{j S_{j}}\right\}$，条件概率 $P\left(X^{(j)}=a_{j l} | Y = c_k )\right.$ 的极大似然估计是：</p><script type="math/tex; mode=display">P\left(X^{(j)}=a_{j l} | Y=c_{k}\right)=\frac{\sum_{i=1}^{N} I\left(x_{i}^{(j)}=a_{j l}, y_{i}=c_{k}\right)}{\sum_{i=1}^{N} I\left(y_{i}=c_{k}\right)}</script><script type="math/tex; mode=display">j=1,2, \cdots, n ; \quad l=1,2, \cdots, S_{j} ; \quad k=1,2, \cdots, K</script><h3 id="3-算法过程"><a href="#3-算法过程" class="headerlink" title="3 算法过程"></a>3 算法过程</h3><p>(1) 计算先验概率和条件概率（见2，极大似然估计部分）</p><p>(2) 对于给定实例 $x=\left(x^{(1)}, x^{(2)}, \cdots, x^{(n)}\right)^{\mathrm{T}}$计算，取最大值</p><script type="math/tex; mode=display">y = f(x)=argmin_{c_k} \frac{P\left(Y=c_{k}\right) \prod_{j}^n P\left(X^{(j)}=x^{(j)} | Y=c_{k}\right)}{\sum_{k} P\left(Y=c_{k}\right) \prod_{j}^n P\left(X^{(j)}=x^{(j)} | Y=c_{k}\right)}, \quad k=1,2, \cdots, K</script>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h3 id=&quot;1-简介&quot;&gt;&lt;a href=&quot;#1-简介&quot; class=&quot;headerlink&quot; title=&quot;1 简介&quot;&gt;&lt;/a&gt;1 简介&lt;/h3&gt;&lt;p&gt;朴素贝叶斯法基于&lt;u&gt;贝叶斯定理&lt;/u&gt;与&lt;u&gt;特征条件独立&lt;/u&gt;假设的分类方法。对于给定的训练数据集，首先基于特征条件独
      
    
    </summary>
    
    
      <category term="机器学习" scheme="http://yoursite.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="机器学习" scheme="http://yoursite.com/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>感知机 &amp; KNN</title>
    <link href="http://yoursite.com/2018/12/09/20191224%E6%84%9F%E7%9F%A5%E6%9C%BA%E4%B8%8EKNN/"/>
    <id>http://yoursite.com/2018/12/09/20191224%E6%84%9F%E7%9F%A5%E6%9C%BA%E4%B8%8EKNN/</id>
    <published>2018-12-09T10:52:35.000Z</published>
    <updated>2019-12-25T09:45:18.376Z</updated>
    
    <content type="html"><![CDATA[<h3 id="1-感知机"><a href="#1-感知机" class="headerlink" title="1 感知机"></a>1 感知机</h3><h4 id="1-1-简介"><a href="#1-1-简介" class="headerlink" title="1.1 简介"></a>1.1 简介</h4><p>感知机是二分类<u>线性分类</u>模型，感知机对输入空间中将实例划分为正负两类的分离超平面，属于判别模型。使用基于误分类的损失函数，利用梯度下降法对损失函数进行最小化。</p><p>感知机：</p><script type="math/tex; mode=display">f(x)=\operatorname{sign}(w \cdot x+b)</script><script type="math/tex; mode=display">\operatorname{sign}(x)=\left\{\begin{array}{ll}{+1,} & {x \geqslant 0} \\ {-1,} & {x<0}\end{array}\right.</script><p><img src="/images/20181204InceptionMachine.jpg" alt="20181204InceptionMachine"></p><h4 id="1-2-学习策略"><a href="#1-2-学习策略" class="headerlink" title="1.2 学习策略"></a>1.2 学习策略</h4><p>损失函数的自然选择是误分类点的总数（但这样$w,b$不是连续可导函数，不易优化）</p><p>故采用误分类点到超平面$S$的总距离：</p><script type="math/tex; mode=display">\frac{1}{\|w\|}\left|w \cdot x_{0}+b\right|</script><p>由于对于一个误分类的数据$(x_i,y_i)$来说：</p><script type="math/tex; mode=display">-y_i(w \cdot x_i + b) > 0</script><p>因此感知机的损失函数是（忽略常数）：</p><script type="math/tex; mode=display">L(w, b)=-\sum_{x_{i} \in M} y_{i}\left(w \cdot x_{i}+b\right)</script><h4 id="1-3-随机梯度下降"><a href="#1-3-随机梯度下降" class="headerlink" title="1.3 随机梯度下降"></a>1.3 随机梯度下降</h4><p>一次随机选取一个误分类点使其梯度下降。</p><p>损失函数的梯度：</p><script type="math/tex; mode=display">\nabla_{w} L(w, b)=-\sum_{x_{i} \in M} y_{i} x_{i}</script><script type="math/tex; mode=display">\nabla_{b} L(w, b)=-\sum_{x_{i} \in M} y_{i}</script><p>选取一个误分类点$(x_i,y_i)$进行更新w，b。</p><script type="math/tex; mode=display">w = w + \alpha y_i x_i , b = b + \alpha y_i</script><h4 id="1-4-感知机算法过程"><a href="#1-4-感知机算法过程" class="headerlink" title="1.4 感知机算法过程"></a>1.4 感知机算法过程</h4><p>输入：训练数据集$T=\left\{\left(x_{1}, y_{1}\right),\left(x_{2}, y_{2}\right), \cdots,\left(x_{N}, y_{N}\right)\right\}$，其中$x_{i} \in \mathcal{X}=\mathbf{R}^{n}, y_i \in y=\{-1,+1\}, i=1,2, \cdots, N$ 。学习率 $\alpha$，</p><p>输出： 求解感知机模型 $f(x) = sign(w \cdot x + b)$</p><p>(1) 选取初值$w_0,b_0$</p><p>(2) 在训练数据中选取数据 $(x_i,y_i)$</p><p>(3) 如果$y_{i}\left(w \cdot x_{i}+b\right) \leqslant 0$ ，则 $w = w + \alpha y_i x_i, b = b + \alpha y_i$</p><h3 id="2-K近邻KNN"><a href="#2-K近邻KNN" class="headerlink" title="2 K近邻KNN"></a>2 K近邻KNN</h3><h4 id="2-1-简介"><a href="#2-1-简介" class="headerlink" title="2.1 简介"></a>2.1 简介</h4><p>KNN是一种基本的<u>分类</u>与回归方法。KNN法假设给定一个训练数据集，其中实例类别已定。分类时，对新的实例根据其K个最近邻的训练实例的类别，通过多数表决来预测。</p><p>基本要素：K值的选择，距离度量，分类决策规则</p><h4 id="2-2-K近邻法算法"><a href="#2-2-K近邻法算法" class="headerlink" title="2.2 K近邻法算法"></a>2.2 K近邻法算法</h4><p>输入：训练数据集$T=\left\{\left(x_{1}, y_{1}\right),\left(x_{2}, y_{2}\right), \cdots,\left(x_{N}, y_{N}\right)\right\}$，其中 $x_{i} \in \mathcal{X}=\mathbf{R}^{n}, y_i \in y=\{c_1,c_2,\cdot \cdot, c_k\}, i=1,2, \cdots, N$</p><p>输出：新实例 $x$ 所属的类别 $y$</p><p>(1) 根据给定的距离度量，在训练集中找出与x最临近的k个点，涵盖这k个点的x的邻域记作$N_k(x)$</p><p>(2) 在$N_k(x)$中根据分类决策规则（如多数表决）来判定x的类别y</p><script type="math/tex; mode=display">y=\arg \max _{c_{j}} \sum_{x_{i} \in N_{k}(x)} I\left(y_{i}=c_{j}\right), \quad i=1,2, \cdots, N ; j=1,2, \cdots, K</script><p>$I$是指示函数，当$y_i = c_j$ 时为1。</p><h4 id="2-3-距离度量"><a href="#2-3-距离度量" class="headerlink" title="2.3 距离度量"></a>2.3 距离度量</h4><p>距离是两个点相似度的反映。设特征空间$\mathcal{X}$是n维实数向量空间$\mathbf{R}^{n}$, </p><script type="math/tex; mode=display">x_{i}, x_{j} \in \mathcal{X}, x_{i}=\left(x_{i}^{(1)}, x_{i}^{(2)}, \cdots, x_{i}^{(n)}\right)^{\mathrm{T}}</script><script type="math/tex; mode=display">x_{j}=\left(x_{j}^{(1)}, x_{j}^{(2)}, \cdots, x_{j}^{(n)}\right)^{\mathrm{T}}</script><p>则P范数距离是：</p><script type="math/tex; mode=display">L_p(x_i,x_j) = ( \sum_{l=1}^n |x_i^{(l)} - x_j^{(l)}|^p)^{\frac{1}{p}}</script><p>p=1 曼哈顿距离，p=2 欧氏距离，p=$\infty$，是切比雪夫距离，各个坐标距离的最大值。</p><script type="math/tex; mode=display">L_{\infty}\left(x_{i}, x_{j}\right)=\max _{l}\left|x_{i}^{(l)}-x_{j}^{(l)}\right|</script><h4 id="2-4-K值的影响"><a href="#2-4-K值的影响" class="headerlink" title="2.4 K值的影响"></a>2.4 K值的影响</h4><p>1，k较小时，使用较小的邻域中的训练实例进行预测，“学习”的近似误差减小，但是估计误差增大。即预测结果对邻近点非常敏感，如果邻近的点是噪声则会预测出错（容易过拟合）。</p><h4 id="2-5-分类决策规则"><a href="#2-5-分类决策规则" class="headerlink" title="2.5 分类决策规则"></a>2.5 分类决策规则</h4><p>一般多数表决。误分类的概率是：</p><script type="math/tex; mode=display">P(Y \neq f(X))=1-P(Y=f(X))</script><p>则误分类率是：</p><script type="math/tex; mode=display">\frac{1}{k} \sum_{x_{i} \in N_{k}(x)} I\left(y_{i} \neq c_{j}\right)=1-\frac{1}{k} \sum_{x_{i} \in N_{k}(x)} I\left(y_{i}=c_{j}\right)</script><h4 id="2-6-k-d-tree"><a href="#2-6-k-d-tree" class="headerlink" title="2.6 k-d tree"></a>2.6 k-d tree</h4><p>为了提高k近邻的搜索效率。用特殊的结构存储训练数据，减少计算距离的次数。构造kd树相当于不断地用垂直于坐标轴的超平面将k维空间切分（递归），构成一系列的k维超矩形矩阵区域。（李航书P53）</p><p>搜索：首先找到包含目标点的叶节点，然后从该叶节点出发依次回退到父节点，不断查找与目标点最邻近的节点。当确定不存在更近的结点时终止。</p><h3 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h3><p>1，《统计学习方法》李航</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h3 id=&quot;1-感知机&quot;&gt;&lt;a href=&quot;#1-感知机&quot; class=&quot;headerlink&quot; title=&quot;1 感知机&quot;&gt;&lt;/a&gt;1 感知机&lt;/h3&gt;&lt;h4 id=&quot;1-1-简介&quot;&gt;&lt;a href=&quot;#1-1-简介&quot; class=&quot;headerlink&quot; title=&quot;1
      
    
    </summary>
    
    
      <category term="机器学习" scheme="http://yoursite.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="机器学习" scheme="http://yoursite.com/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>SVM推导</title>
    <link href="http://yoursite.com/2018/12/09/20191209SVM%E6%8E%A8%E5%AF%BC/"/>
    <id>http://yoursite.com/2018/12/09/20191209SVM%E6%8E%A8%E5%AF%BC/</id>
    <published>2018-12-09T01:26:32.000Z</published>
    <updated>2019-12-25T12:54:54.587Z</updated>
    
    <content type="html"><![CDATA[<h3 id="1-简介"><a href="#1-简介" class="headerlink" title="1 简介"></a>1 简介</h3><p>标签：二分类模型，特征空间上的间隔最大的线性分类器，核技巧（非线性问题），求解凸二次规划问题。</p><p>三类：线性可分支持向量机与硬间隔最大化，线性支持向量机与软间隔最大化，非线性支持向量机与核函数。</p><h3 id="2-线性可分支持向量机"><a href="#2-线性可分支持向量机" class="headerlink" title="2 线性可分支持向量机"></a>2 线性可分支持向量机</h3><p>学习的目标是在特征空间中找出一个分离超平面，将实例分到不同的类。</p><script type="math/tex; mode=display">wx + b = 0</script><p>$w$代表法向量，指向的一般是正类；$b$是截距。</p><p>利用误分类最小的策略求得分离超平面。利用间隔最大化求最优分离超平面。</p><p>在超平面确定的情况下，$|wx+b|$能够相对表示点$x$距离超平面的远近，$y (wx+b)$的符号是否一致表示分类是否正确。</p><h4 id="2-1-函数间隔"><a href="#2-1-函数间隔" class="headerlink" title="2.1 函数间隔"></a>2.1 函数间隔</h4><p>1，函数间隔  $\hat{\gamma}_{i}=y_{i}\left(w \cdot x_{i}+b\right)$，超平面关于训练数据$T$ 的函数间隔 $\hat{\gamma}=\min _{i=1, \cdots, N} \hat{\gamma}_{i}$</p><p>函数间隔可以表示分类预测的正确性及确信度。</p><p>2，几何间隔：</p><p>对分离超平面的法向量 $w$加上某些约束，如规范化 $||w||=1$，使得间隔确定，这时函数间隔为<strong>几何间隔</strong>。</p><p>给定训练集$T$ 和超平面$(w,b)$，定义超平面关于样本点$(x_i,y_i)$的几何间隔是：</p><p><img src="/images/20191209margin.jpg" alt="20191209几何间隔"></p><script type="math/tex; mode=display">\gamma_{i}=y_{i}\left(\frac{w}{\|w\|} \cdot x_{i}+\frac{b}{\|w\|}\right)</script><h4 id="2-2-间隔最大化"><a href="#2-2-间隔最大化" class="headerlink" title="2.2 间隔最大化"></a>2.2 间隔最大化</h4><p>1，SVM基本思想</p><p>求解能够正确划分训练数据集并且几何间隔最大的分离超平面。</p><script type="math/tex; mode=display">\max _{w, b} \frac{\hat{\gamma}}{\|w\|}</script><script type="math/tex; mode=display">\text { s.t. } \quad y_{i}\left(w \cdot x_{i}+b\right) \geqslant \hat{\gamma}, \quad i=1,2, \cdots, N</script><p><strong>函数间隔$\hat{\gamma}$ 不影响最优化问题的解，可以得到下面的线性可分支持向量机学习的最优化问题，凸二次规划问题，有最优解且唯一。（原始最优化问题）</strong> （备注，判断一个问题是否是凸问题，<a href="https://www.zhihu.com/question/334515180" target="_blank" rel="noopener">凸优化</a>）</p><script type="math/tex; mode=display">\min _{w, b} \frac{1}{2}\|w\|^{2}</script><script type="math/tex; mode=display">\text { s.t. } \quad y_{i}\left(w \cdot x_{i}+b\right)-1 \geqslant 0, \quad i=1,2, \cdots, N</script><p>求解得到分离超平面：</p><script type="math/tex; mode=display">w^{*}·x+b^{*}=0</script><p>分类决策函数是：</p><script type="math/tex; mode=display">f(x)=\operatorname{sign}\left(w^{*} \cdot x+b^{*}\right)</script><p>2，支持向量</p><p>在线性可分情况下，训练数据集的样本点中与分离超平面距离最近的样本点的实例称为支持向量。支持向量是使约束$\text { s.t. } \quad y_{i}\left(w \cdot x_{i}+b\right)-1 = 0$ 成立。</p><p><img src="/images/20191209SupportVector.jpg" alt="20191209SupportVector"></p><p>间隔等于$\frac{2}{||w||}$</p><h4 id="2-3-对偶问题"><a href="#2-3-对偶问题" class="headerlink" title="2.3 对偶问题"></a>2.3 对偶问题</h4><p>通过求解对偶问题得到原始问题的最优解。</p><p><strong>首先构建拉格朗日函数</strong>，不等式约束引入拉格朗日乘子 $\alpha_{i} \geqslant 0 , i=1,2, \cdots, N$</p><script type="math/tex; mode=display">L(w, b, \alpha)=\frac{1}{2}\|w\|^{2}-\sum_{i=1}^{N} \alpha_{i} y_{i}\left(w \cdot x_{i}+b\right)+\sum_{i=1}^{N} \alpha_{i}</script><p>其中 $\alpha=\left(\alpha_{1}, \alpha_{2}, \cdots, \alpha_{N}\right)^{\mathrm{T}}$ 是拉格朗日乘子向量</p><p><strong>需求$\max _{\alpha} \min _{w, b} L(w, b, \alpha)$</strong></p><p>Step1，先求$\min _{w, b} L(w, b, \alpha)$</p><p>拉格朗日函数 $L(w, b, \alpha) $ 分别对$w,b$求偏导并令其等于0。</p><script type="math/tex; mode=display">\nabla_{w} L(w, b, \alpha)=w-\sum_{i=1}^{N} \alpha_{i} y_{i} x_{i}=0</script><script type="math/tex; mode=display">\nabla_{b} L(w, b, \alpha)=-\sum_{i=1}^{N} \alpha_{i} y_{i}=0</script><p>代入回拉格朗日函数中 $L(w,b,\alpha)$ 并化简：</p><script type="math/tex; mode=display">L(w, b, \alpha)=\frac{1}{2} \sum_{i=1}^{N} \sum_{j=1}^{N} \alpha_{i} \alpha_{j} y_{i} y_{j}\left(x_{i} \cdot x_{j}\right)-\sum_{i=1}^{N} \alpha_{i} y_{i}\left(\left(\sum_{j=1}^{N} \alpha_{j} y_{j} x_{j}\right) \cdot x_{i}+b\right)+\sum_{i=1}^{N} \alpha_{i}</script><p>化简推导过程见[5]，也可以代入简单的例子去看，化简得到：</p><script type="math/tex; mode=display">\min _{w, b} L(w, b, \alpha)=-\frac{1}{2} \sum_{i=1}^{N} \sum_{j=1}^{N} \alpha_{i} \alpha_{j} y_{i} y_{j}\left(x_{i} \cdot x_{j}\right)+\sum_{i=1}^{N} \alpha_{i}</script><p>Step2，求$\min _{w, b} L(w, b, \alpha)$ 对$\alpha$的极大值，<strong>即是对偶问题</strong>：</p><script type="math/tex; mode=display">\max _{\alpha}-\frac{1}{2} \sum_{i=1}^{N} \sum_{j=1}^{N} \alpha_{i} \alpha_{j} y_{i} y_{j}\left(x_{i} \cdot x_{j}\right)+\sum_{i=1}^{N} \alpha_{i}</script><p>转换为求极小</p><script type="math/tex; mode=display">\min _{\alpha} \frac{1}{2} \sum_{i=1}^{N} \sum_{j=1}^{N} \alpha_{i} \alpha_{j} y_{i} y_{j}\left(x_{i} \cdot x_{j}\right)-\sum_{i=1}^{N} \alpha_{i}</script><script type="math/tex; mode=display">\text { s.t. } \quad \sum_{i=1}^{N} \alpha_{i} y_{i}=0 , \alpha_i \geqslant 0</script><p>注意：为什么线性规划中求解原始问题可以转为求解对偶问题，对偶问题有良好的性质。对偶问题的对偶是原问题，无论原始问题是否是凸对偶问题都是凸优化问题，对偶问题可以给出原始问题的一个下界，满足一定条件时原始问题的解与对偶问题的解是完全等价的（备注 <a href="https://zhuanlan.zhihu.com/p/31131842" target="_blank" rel="noopener">拉格朗日对偶问题</a>）。</p><p>Step3，根据KKT条件求得 $w^{\star}$ 与 $b^{\star}$</p><p>过程如下，设 $\alpha^{\star}=\left(\alpha_{1}^{\star}, \alpha_{2}^{\star}, \cdots, \alpha_{N}^{\star}\right)^{\mathrm{T}}$ 是对偶最优化问题的解。根据原问题的不等式约束，KKT条件成立。</p><script type="math/tex; mode=display">\nabla_{w} L\left(w^{\star}, b^{\star}, \alpha^{\star}\right)=w^{\star}-\sum_{i=1}^{N} \alpha_{i}^{\star} y_{i} x_{i}=0</script><script type="math/tex; mode=display">\nabla_{b} L\left(w^{\star}, b^{\star}, \alpha^{\star}\right)=-\sum_{i=1}^{N} \alpha_{i}^{\star} y_{i}=0</script><script type="math/tex; mode=display">\alpha_{i}^{\star}\left(y_{i}\left(w^{\star} \cdot x_{i}+b^{\star}\right)-1\right)=0, \quad i=1,2, \cdots, N</script><script type="math/tex; mode=display">y_{i}\left(w^{\star} \cdot x_{i}+b^{\star}\right)-1 \geqslant 0, \quad i=1,2, \cdots, N</script><script type="math/tex; mode=display">\alpha_{i}^{\star} \geqslant 0, \quad i=1,2, \cdots, N</script><p>由此可得:</p><script type="math/tex; mode=display">w^{*}=\sum_{i} \alpha_{i}^{\star} y_{i} x_{i}</script><p>至少有一个$\alpha_i^{\star} &gt; 0$，将$w^*$ 代入$y_{j}\left(w^{\star} \cdot x_{j}+b^{\star}\right)-1=0$ （注意 $y_j^2 = 1$）得:</p><script type="math/tex; mode=display">b^{\star}=y_{j}-\sum_{i=1}^{N} \alpha_{i}^{\star} y_{i}\left(x_{i} \cdot x_{j}\right)</script><p>最终分类决策函数是：</p><script type="math/tex; mode=display">f(x)=\operatorname{sign}\left(\sum_{i=1}^{N} \alpha_{i}^{\star} y_{i}\left(x \cdot x_{i}\right)+b^{\star}\right)</script><p><img src="/images/20191209SVMAlgorithm.jpg" alt="20191209SVMAlgorithm"></p><p>《李航》书P125例子好</p><h3 id="3-线性支持向量机与软间隔最大化"><a href="#3-线性支持向量机与软间隔最大化" class="headerlink" title="3 线性支持向量机与软间隔最大化"></a>3 线性支持向量机与软间隔最大化</h3><p>线性不可分问题（即某些样本点不满足函数间隔大于等于1的约束条件），用软间隔最大化（引入松弛变量 ）:</p><script type="math/tex; mode=display">y_i(w \cdot x_i + b) \geqslant 1 - \xi_i</script><p>原始问题：</p><script type="math/tex; mode=display">\min _{w, b} \frac{1}{2}\|w\|^{2} + C \sum_{i=1}^N \xi_i</script><script type="math/tex; mode=display">\text { s.t. } \quad y_{i}\left(w \cdot x_{i}+b\right) \geqslant 1 - \xi_i, \quad i=1,2, \cdots, N</script><script type="math/tex; mode=display">\xi_i \geqslant 0,i=1,2, \cdots, N</script><p>对偶问题：</p><script type="math/tex; mode=display">\min _{\alpha} \frac{1}{2} \sum_{i=1}^{N} \sum_{j=1}^{N} \alpha_{i} \alpha_{j} y_{i} y_{j}\left(x_{i} \cdot x_{j}\right)-\sum_{i=1}^{N} \alpha_{i}</script><script type="math/tex; mode=display">\text { s.t. } \quad \sum_{i=1}^{N} \alpha_{i} y_{i}=0</script><script type="math/tex; mode=display">0 \leq \alpha_i \leq C, i=1,2, \cdots, N</script><h3 id="4-非线性支持向量机"><a href="#4-非线性支持向量机" class="headerlink" title="4 非线性支持向量机"></a>4 非线性支持向量机</h3><p>核技巧。例子从椭圆变到线性可分，设原空间：$\mathcal{X} \subset \mathbf{R}^{2}, x=\left(x^{(1)}, x^{(2)}\right)^{\mathrm{T}} \in \mathcal{X}$，新空间 $\mathcal{Z} \subset \mathbf{R}^{2}, z=\left(z^{(1)}, z^{(2)}\right)^{\mathrm{T}} \in \mathcal{Z}$，从原空间到新空间的映射：$z=\phi(x)=\left(\left(x^{(1)}\right)^{2},\left(x^{(2)}\right)^{2}\right)^{\mathrm{T}}$</p><h4 id="4-1-核函数"><a href="#4-1-核函数" class="headerlink" title="4.1 核函数"></a>4.1 核函数</h4><p>核函数定义</p><p>设$\mathcal{X}$ 是输入空间（欧式空间$R^n$ 的子集或者离散集合），设$\mathcal{H}$为特征k空间（希尔伯特空间），如果存在一个映射：</p><script type="math/tex; mode=display">\phi(x): \mathcal{X} \rightarrow \mathcal{H}</script><p>使得对所有$x, z \in \mathcal{X}$ ，函数K(x,z) 满足条件 $K(x, z)=\phi(x) \cdot \phi(z)$ 则K为核函数，$\phi(x)$是映射函数。</p><p>对偶问题的目标函数中内积用核函数来替代表示为：</p><script type="math/tex; mode=display">W(\alpha)=\frac{1}{2} \sum_{i=1}^{N} \sum_{j=1}^{N} \alpha_{i} \alpha_{j} y_{i} y_{j} K\left(x_{i}, x_{j}\right)-\sum_{i=1}^{N} \alpha_{i}</script><p>通常所说的核函数就是正定核函数。定义映射，定义内积使其成为内积空间（证明定义的运算是内积，证明其加法和数乘是封闭的。），将内积空间完备化为希尔伯特空间。</p><p>常用的核函数：</p><p><img src="/images/20191209KernelFunction.jpg" alt="20191209KernelFunction"></p><h4 id="4-2-序列最小最优化算法"><a href="#4-2-序列最小最优化算法" class="headerlink" title="4.2 序列最小最优化算法"></a>4.2 序列最小最优化算法</h4><p>SMO算法包括两部分：求解两个变量二次规划的解析方法和选择变量的启发式方法。</p><h3 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h3><p>1, <a href="https://www.zhihu.com/question/334515180" target="_blank" rel="noopener">https://www.zhihu.com/question/334515180</a></p><p>2，陈宝林，最优化理论与算法</p><p>3，李航《统计学习方法》</p><p>4，<a href="https://zhuanlan.zhihu.com/p/31131842" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/31131842</a></p><p>5，优秀博文 <a href="https://blog.csdn.net/v_JULY_v/article/details/7624837" target="_blank" rel="noopener">https://blog.csdn.net/v_JULY_v/article/details/7624837</a></p><p>6，支持向量机通俗导论(理解 SVM 的三层境界)  ，这个也讲的好</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h3 id=&quot;1-简介&quot;&gt;&lt;a href=&quot;#1-简介&quot; class=&quot;headerlink&quot; title=&quot;1 简介&quot;&gt;&lt;/a&gt;1 简介&lt;/h3&gt;&lt;p&gt;标签：二分类模型，特征空间上的间隔最大的线性分类器，核技巧（非线性问题），求解凸二次规划问题。&lt;/p&gt;
&lt;p&gt;三类：线性可分
      
    
    </summary>
    
    
      <category term="机器学习" scheme="http://yoursite.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="机器学习" scheme="http://yoursite.com/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>LR原理</title>
    <link href="http://yoursite.com/2018/12/08/20191208LR%E5%8E%9F%E7%90%86/"/>
    <id>http://yoursite.com/2018/12/08/20191208LR%E5%8E%9F%E7%90%86/</id>
    <published>2018-12-08T02:59:11.000Z</published>
    <updated>2019-12-25T12:07:43.560Z</updated>
    
    <content type="html"><![CDATA[<h3 id="1、线性回归"><a href="#1、线性回归" class="headerlink" title="1、线性回归"></a>1、线性回归</h3><h4 id="1-1-单变量线性回归"><a href="#1-1-单变量线性回归" class="headerlink" title="1.1 单变量线性回归"></a>1.1 单变量线性回归</h4><p>x ——&gt; hypothesis（假设）——&gt; y，此处假设为线性函数，y输出为数值（若是分类则为0或1）</p><script type="math/tex; mode=display">h_{\theta}{(x)} = \theta^Tx</script><p>为了让hypothesis尽量根据数据拟合好曲线，需要设计损失函数，并对此损失函数优化。损失函数是参数$\theta$ 的 Cost function: </p><script type="math/tex; mode=display">J(\theta)=\min \frac{1}{2m} \sum_{i=1}^{m}\left(h_{\theta}(x)-y\right)^{2}</script><p>优化损失函数用到梯度下降法：</p><p>为了 $min J(\theta)$，我们采用随机梯度下降方法。（其实也可以用一些矩阵直接计算的方法，最优化里的牛顿法、BFGS等等）</p><p>repeat until convergence{</p><script type="math/tex; mode=display">\theta_{j}=\theta_{j}-\alpha \frac{\partial}{\partial \theta_{j}} J(\theta) \quad(for j=0 \cdots)</script><p>}</p><p>展开就是:</p><script type="math/tex; mode=display">\theta_j = \theta_j - \alpha \frac{1}{m} \sum_{i=1}^m (h_\theta(x^{(i)})- y^{(i)}) \frac{\partial h_\theta(x)}{\partial \theta_{j}}</script><p><strong>注意</strong>：</p><p>1，学习率影响了梯度下降的步长，一般越大下降越快（但如果最初就在靠近局部最优处，则容易震荡发散），一般设置在0.001~0.003, 取比最大值稍小一点的值即可。</p><p>2，不同的初始$\theta$ 可能下降到不同的局部最优点（因此，我们希望损失函数最好是凸函数，线性回归的$J$就是凸函数，是碗面）。</p><p>3，数据处理的小技巧，将特征归一化到0-1或者-1-1可以避免量纲影响 x- mean / std。</p><p>4，<strong>特征工程非常重要</strong>，特征组合，平方，开根号等等。</p><h3 id="2，逻辑回归LR"><a href="#2，逻辑回归LR" class="headerlink" title="2，逻辑回归LR"></a>2，逻辑回归LR</h3><p>逻辑回归是用来解决二分类问题的机器学习方法，用于估计某种事物的可能性。</p><p>逻辑回归中x ——&gt; hypothesis（假设）——&gt; y，sigmoid函数将预测值转换为0-1之间的值。</p><script type="math/tex; mode=display">0 \leq h_{\theta}(x) \leqslant 1</script><script type="math/tex; mode=display">h_{\theta}(x)=g\left(\theta^{\top} x\right)</script><script type="math/tex; mode=display">g(z)=\frac{1}{1+e^{-z}}</script><p>解释：$h_{\theta}(x)$ 是对输入x，y=1的概率估计，$h_{\theta}(x)=P(y=1 / x ; \theta)$ 给定特征x与参数$\theta$时，y=1的概率。也可以这样理解，一个事件的发生几率指的是该事件发生的概率p与该事件不发生的概率1-p的比值。</p><script type="math/tex; mode=display">logit(p) = \log{\frac{p}{1-p}}</script><script type="math/tex; mode=display">\log \frac{P(Y=1|x)}{1-P(Y=1|x)} = \theta^T x</script><p>LR中，输出Y=1的对数几率是输入x的线性函数。</p><p>损失函数这里有所不同，因为sigmoid函数代入到平方误差中得到$J$ 是非凸函数，所以cost function用的是交叉熵（信息量中度量不确定性的度量）。</p><script type="math/tex; mode=display">J\left(h_{\theta}(x), y\right)=-\frac{1}{m} [ \sum_{i=1}^m y^{(i)} \log \left(h_{\theta}(x^{(i)})\right) + (1-y^{(i)}) \log \left(1-h_{\theta}(x^{(i)})\right) ]</script><p>这个非常像似然函数。</p><p>LR的梯度下降公式（对各个参数的偏导 or 链式求导），因为sigmoid函数求导特殊g(z)’ = g(z) (1-g(z))’。</p><script type="math/tex; mode=display">\theta_j = \theta_j - \alpha \frac{\partial}{\partial \theta_j} J(\theta)</script><script type="math/tex; mode=display">\theta_{j}=\theta_j-\alpha\frac{1}{m} \sum_{i=1}^{m}\left(h_{\theta}\left(x^{(i)}\right)-y^{(i)}\right) x_{j}^{(i)}</script><p>推导过程：</p><script type="math/tex; mode=display">\frac{\partial}{\partial \theta_j} J(\theta) =-\frac{1}{m} \sum_{i=1}^{m}\left[y^{(i)} \frac{1}{\left.h_{\theta}\left(x^{(i)}\right)\right)} \frac{\partial}{\partial \theta_{j}} h_{\theta}\left(x^{(i)}\right)-\left(1-y^{(i)}\right) \frac{1}{1-h_{\theta}\left(x^{(i)}\right)} \frac{\partial}{\partial \theta_{j}} h_{\theta}\left(x^{(i)}\right)\right]</script><script type="math/tex; mode=display">\frac{\partial}{\partial \theta_j} h_\theta(x^{(i)}) = \frac{e^{-\theta^{T} x^{(i)}}}{\left(1+e^{-\theta^{T} x^{(i)}}\right)^{2}} \frac{\partial}{\partial \theta_{j}} \theta^{T} x^{(i)} = g\left(\theta^{T} x^{(i)}\right)\left(1-g\left(\theta^{T} x^{(i)}\right)\right) x_{j}^{(i)}</script><p>最后代入化简即可。</p><p>其实逻辑回归还可以用于多分类问题上，分别拟合三个分类器$h_{\theta}^{(i)}(x)$，选择 $max {h_{\theta}^{(i)}(x)}$的类i。</p><p><strong>注意：</strong></p><p>优点：</p><p>1，对逻辑回归来说，多重共线性并不是问题，它可以结合L2正则化来解决</p><p>2，属于判别式模型</p><p>3，在线梯度下降算法-online gradient descent</p><p>4，便利的观测样本概率分数</p><p>缺点</p><p>1，特征空间很大时，逻辑回归的性能不是很好（不能很好地处理大量多类特征或变量，one hot很大）</p><p>2，容易欠拟合，一般准确度不太高；</p><p>3，对于非线性特征，需要进行转换。</p><h3 id="3，过拟合与正则化"><a href="#3，过拟合与正则化" class="headerlink" title="3，过拟合与正则化"></a>3，过拟合与正则化</h3><h4 id="3-1-欠拟合与过拟合"><a href="#3-1-欠拟合与过拟合" class="headerlink" title="3.1 欠拟合与过拟合"></a>3.1 欠拟合与过拟合</h4><p>欠拟合：模型过于简单，underfit，带来高偏差high bias，就是说模型偏见很强；</p><p>过拟合：模型复杂，overfit，太过于拟合训练数据，经验误差虽小但结构误差大，无法拟合新数据。</p><p><img src="/images/20191209overfit.jpg" alt="20191209overfit"></p><p>避免过拟合的方法：数据增强more data，简化模型（早停，限制权值正则化，多种模型Bagging，Boosting），增加噪声，集成ensemble，贝叶斯等。</p><h4 id="3-2-正则化"><a href="#3-2-正则化" class="headerlink" title="3.2 正则化"></a>3.2 正则化</h4><p>正则化，损失部分尽量拟合数据，后面部分尽量保持参数较小，起到正则化作用。</p><p>线性回归的损失函数：</p><script type="math/tex; mode=display">J(\theta) = \frac{1}{2m}[\sum_{i=1}^{m}(h_\theta(x^{(i)}) - y^{(i)} )^2 + \lambda \sum_{i=1}^{n} \theta_j^2]</script><p>逻辑回归的损失函数</p><script type="math/tex; mode=display">J(\theta)=-\left[\frac{1}{m} \sum_{i=1}^{m} y^{(i)} \log \left(h_{\theta}(x^{(i)}\right)+\left(1-y^{(i)}\right) \log \left(1-h_{\theta}\left(x^{(i)}\right)\right)\right] + \frac{\lambda}{2m} \sum_{j=1}^n\theta_j^2</script><p>另外可以参考 <a href="https://zhuanlan.zhihu.com/p/25707761" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/25707761</a></p><h3 id="4，优化方法复习"><a href="#4，优化方法复习" class="headerlink" title="4，优化方法复习"></a>4，优化方法复习</h3><h4 id="4-1-最速下降法即负梯度方向"><a href="#4-1-最速下降法即负梯度方向" class="headerlink" title="4.1 最速下降法即负梯度方向"></a>4.1 最速下降法即负梯度方向</h4><script type="math/tex; mode=display">f(x_k+\alpha \vec P) = f(x_k) + \alpha P^T \nabla f(x_k) + o(\alpha)</script><script type="math/tex; mode=display">min{f(x_k+\alpha \vec P)}</script><p>则需要$min{P^T \nabla f(x_k)}$</p><script type="math/tex; mode=display">P = -\frac{\nabla f(x_k)}{||\nabla f(x_k)||}</script><p>此处$\alpha$是学习率</p><p>一般来说，损失函数偏碗状的时候，比较圆的时候，下降比较快。如果，函数形状椭圆形，会来回震荡着走。</p><h4 id="4-2-牛顿法"><a href="#4-2-牛顿法" class="headerlink" title="4.2 牛顿法"></a>4.2 牛顿法</h4><p>牛顿法考虑二阶导信息</p><script type="math/tex; mode=display">f\left(x_{k} + P\right)=f\left(x_{k}\right)+p^T \nabla f_{k}+\frac{1}{2} p^{T} \nabla^{2} f_{k} P</script><p>则$P = - \nabla^2 f(x_k)^{-1} \nabla f(x_k)$</p><p>此处二阶导大于0，即要求$\nabla^2 f(x_k)$ 正定。</p><p>总结：逻辑回归主要是增加了一个sigmoid函数，将预测值映射为概率。为了避免损失函数变为非凸函数，损失函数变为对数损失函数。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h3 id=&quot;1、线性回归&quot;&gt;&lt;a href=&quot;#1、线性回归&quot; class=&quot;headerlink&quot; title=&quot;1、线性回归&quot;&gt;&lt;/a&gt;1、线性回归&lt;/h3&gt;&lt;h4 id=&quot;1-1-单变量线性回归&quot;&gt;&lt;a href=&quot;#1-1-单变量线性回归&quot; class=&quot;header
      
    
    </summary>
    
    
      <category term="机器学习" scheme="http://yoursite.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="机器学习" scheme="http://yoursite.com/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>20180318《乔布斯传》——伟大的人和伟大的产品</title>
    <link href="http://yoursite.com/2018/03/18/20180318%E3%80%8A%E4%B9%94%E5%B8%83%E6%96%AF%E4%BC%A0%E3%80%8B%E2%80%94%E2%80%94%E4%BC%9F%E5%A4%A7%E7%9A%84%E4%BA%BA%E5%92%8C%E4%BC%9F%E5%A4%A7%E7%9A%84%E4%BA%A7%E5%93%81/"/>
    <id>http://yoursite.com/2018/03/18/20180318%E3%80%8A%E4%B9%94%E5%B8%83%E6%96%AF%E4%BC%A0%E3%80%8B%E2%80%94%E2%80%94%E4%BC%9F%E5%A4%A7%E7%9A%84%E4%BA%BA%E5%92%8C%E4%BC%9F%E5%A4%A7%E7%9A%84%E4%BA%A7%E5%93%81/</id>
    <published>2018-03-18T04:04:20.000Z</published>
    <updated>2019-12-10T04:06:43.681Z</updated>
    
    <content type="html"><![CDATA[<p>你在使用iPhone或者Mac吗？如果有，那你一定要了解下创造它的人，到底怎样一个人才能创造这样伟大的产品呢。 </p><p>我手里有台Mac，大二的时候，因为看中了它的设计感自己掏钱买了。我喜欢他的简洁，流畅，易用，也喜欢苹果开发者们创造出的各类好玩实用的应用，比如sketch。当我买后，班上静和官霸也跟着买了，哈哈。可见他的吸引力多大，那么这背后这个人该更有吸引力吧。</p><p>记得初中的公开课开课前，有位老师说，乔帮主去世了。当时，我还不知道，但听老师的赞美和惋惜。我才知晓了苹果和他，但未曾多关注，不过这也算是种下了一个好奇心吧。过年，老家没网，也就最适合整天阅读了，我孜孜不倦读完了《乔布斯传》。书中故事很有趣，读的时候我还自己笑了出来，读完共鸣感悟颇多。</p><p>乔帮主的特质很多，<strong>突出的是他的现实扭曲场、优雅简约，精致的细节控、强烈的使命感与控制欲、细节把控、出众的演讲能力、识人用人、向渊博的人摄取知识、注重品牌等等</strong>。当然，人也是有缺陷的，但总觉得他的缺陷变成了促进苹果发展的动力。</p><h3 id="1、现实扭曲场——专注"><a href="#1、现实扭曲场——专注" class="headerlink" title="1、现实扭曲场——专注"></a>1、现实扭曲场——专注</h3><p>他是一个极其专注的人，极其投入的人。在设计MAC的时候，即使是开机多用了10s，他不满意他就会要求员工必须做到开机时间减少。他自己也会深信不疑，于是再难的问题再不可能的事情，他总会使用各种方法，哄骗、安抚、劝说、奉承、威胁等让员工服从并做到。这就是他的现实扭曲力场，是一种自身的坚定，不屈的意志，让现实屈从与自己意图的热切渴望。</p><p> 其中还有个好玩的故事，也有员工们为了在一些极端问题上改变乔帮主的想法，勇于挑战他。于是，苹果公司设立了一个奖项，每年给最能抵抗得住乔布斯的现实扭曲力场的人颁发。我觉得他们真是有趣，公司也真是有活力。</p><p>人最难的就是坚信自己，专注投入。我们的想法都很容易受到身边人和世俗的影响，但其实想法万千，哪有什么标准的原则来评判对错呢。只要坚信自己的想法，专注投入到自己认定的事情上，不断追求极致，我们的现实扭曲力场也将出现。</p><h3 id="2、个人电脑、至繁归于至简、站在科技与设计的前沿交汇点"><a href="#2、个人电脑、至繁归于至简、站在科技与设计的前沿交汇点" class="headerlink" title="2、个人电脑、至繁归于至简、站在科技与设计的前沿交汇点"></a>2、个人电脑、至繁归于至简、站在科技与设计的前沿交汇点</h3><p>1、个人电脑：MAC最初也是诞生于车库，当时乔的心愿是做个人电脑，人人都有的电脑，这个想法甚至先于微软。当时的许多公司的产品大多面向企业级顾客，可见乔的远见。他最先将电脑一体化组装，最先将图形界面引入，最先将触屏技术应用。</p><p>2、苹果的电脑和手机设计得像个艺术品一样，不得不赞叹乔帮主的要求和品味。其实，这些品味和要求都来自于其追求优雅简约、来源于他总是站在技与设计的前沿交汇点去思考感受产品。他具有工匠和艺术设计者的本心。MAC电脑表里如一，即使隐藏的部分也做的漂亮；设计追随情感，设计表达情感；造型优美，细节中充满乐趣，至繁归于至简。这些也是他的产品别出一裁、受欢迎的关键原因。</p><p>3、皮克斯与迪士尼：乔帮主被自己的公司开除期间，他喜欢上了另一个具有艺术特质的行业，他想将科技与动画结合。他欣赏拉塞特这样一位具有绅士气质的艺术家，他尊重他的设计。</p><p>乔带来的技术让皮克斯的动画光影、3D效果更棒，加上电影内涵。他创造了另一个最好的品牌——皮克斯动画。现在我们经常在电影开头，看到迪士尼的城堡后会出现皮克斯和一盏跳跳的台灯。这盏台灯动画《顽皮跳跳灯》是皮克斯第一次参加SIGGRAPH大会展示的短片，并被评为最佳影片。后来甚至迪士尼为了挽救自己的电影地位，也不得不和皮克斯合作了。</p><p>跨界交叉的创新来源于人的心灵想法的跨界，如果不是乔对科技和动画艺术的喜爱，怎么会有皮克斯这么棒的动画公司呢。</p><p> 4、音乐变革+数字中枢</p><p>音乐产品ipod的想法来源是乔帮主对版权的保护、为了自己能听最高质量的音乐。他坚定的去和许多唱片公司、音乐人谈判。于是iPod和iTunes出现。2001年互联网泡沫破裂，计算机被预测变为无聊的东西。这时候乔帮主继续思考，他说个人计算机不会成为边缘产品，而将成为数字中枢，管理音乐图片视频信息等等。他又一次站在科技和人文的位置上，先人一步，有了对数字中枢的设想。iTunes后续逐步销售视频、应用程序、订阅服务，慢慢形成了一个数字中枢，它将苹果代入了数字商业的新时代。</p><p>  不得不说，个人电脑产品，各种手持电子产品，路演，产品发布会，数字战略……这些是影响了当今互联网产业的多少方方面面啊。</p><p>他做的是自己喜欢和希望的事情，热情和思考成就了苹果。</p><h3 id="3、强烈的使命感、细节控"><a href="#3、强烈的使命感、细节控" class="headerlink" title="3、强烈的使命感、细节控"></a>3、强烈的使命感、细节控</h3><p>带团队首先自己要有强烈的使命感和愿景的，团队带头人必定知道要去哪。知道怎么去。</p><p>乔帮主总能构建出宏伟强烈的使命感，他最爱的格言是“过程就是奖励”，MAC团队是有着崇高使命的特殊团队。在使命感之下，痛苦会变成过眼云烟，最后长久的留下人生的巅峰时刻。他把控宏观，同时又及其关注细节。伟大的产品，总是在细节之处体现品牌和价值，因此它可以长久的引领潮流。</p><p>他们每次设计一款产品，会有无数多的模型，每一个试用，并不断对比细节处的不同设计。即使到了发布前，如果不满意，他们也会推迟发布会，重新返工打磨产品。这样的呕心沥血，怎会不诞生出伟大的产品呢？</p><h3 id="4、识人用人——向渊博的人摄取知识"><a href="#4、识人用人——向渊博的人摄取知识" class="headerlink" title="4、识人用人——向渊博的人摄取知识"></a>4、识人用人——向渊博的人摄取知识</h3><p>乔帮主的品味、知识、远见不仅仅来源于自身，他是个非常喜欢结交有才华的人。他和艺术大师、广告大师、动画大师、厉害的工程师都会有许多交流。</p><p>他取百家之所长，不断学习和完善自己，于是他成为了强者。同时他的核心圈子里聚集的也都是真正的强者，相互成就。</p><p>乔布斯，他用大师级手法把理念、科技、艺术融合在了一体，创造了未来。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;你在使用iPhone或者Mac吗？如果有，那你一定要了解下创造它的人，到底怎样一个人才能创造这样伟大的产品呢。 &lt;/p&gt;
&lt;p&gt;我手里有台Mac，大二的时候，因为看中了它的设计感自己掏钱买了。我喜欢他的简洁，流畅，易用，也喜欢苹果开发者们创造出的各类好玩实用的应用，比如sk
      
    
    </summary>
    
    
      <category term="读书笔记" scheme="http://yoursite.com/categories/%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="读书笔记" scheme="http://yoursite.com/tags/%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/"/>
    
      <category term="产品" scheme="http://yoursite.com/tags/%E4%BA%A7%E5%93%81/"/>
    
  </entry>
  
</feed>
