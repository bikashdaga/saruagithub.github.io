---
title: 20191208LR原理
date: 2018-12-08 10:59:11
mathjax: true
tags:
- 机器学习
categories:
- 机器学习
---

### 1、线性回归

#### 1.1 单变量线性回归

x ——> hypothesis（假设）——> y，此处假设为线性函数，y输出为数值（若是分类则为0或1）

$$h_{\theta}{(x)} = \theta^Tx$$

为了让hypothesis尽量根据数据拟合好曲线，需要设计损失函数，并对此损失函数优化。损失函数是参数$\theta$ 的 Cost function: 

$$J(\theta)=\min \frac{1}{2m} \sum_{i=1}^{m}\left(h_{\theta}(x)-y\right)^{2}$$

优化损失函数用到梯度下降法：

为了 $min J(\theta)$，我们采用随机梯度下降方法。（其实也可以用一些矩阵直接计算的方法，最优化里的牛顿法、BFGS等等）

repeat until convergence{

$$\theta_{j}=\theta_{j}-\alpha \frac{\partial}{\partial \theta_{j}} J(\theta) \quad(for j=0 \cdots)$$

}

展开就是:

$$\theta_j = \theta_j - \alpha \frac{1}{m} \sum_{i=1}^m (h_\theta(x^{(i)})- y^{(i)}) \frac{\partial h_\theta(x)}{\partial \theta_{j}}$$

**注意**：

1，学习率影响了梯度下降的步长，一般越大下降越快（但如果最初就在靠近局部最优处，则容易震荡发散），一般设置在0.001~0.003, 取比最大值稍小一点的值即可。

2，不同的初始$\theta$ 可能下降到不同的局部最优点（因此，我们希望损失函数最好是凸函数，线性回归的$J$就是凸函数，是碗面）。

3，数据处理的小技巧，将特征归一化到0-1或者-1-1可以避免量纲影响 x- mean / std。

4，**特征工程非常重要**，特征组合，平方，开根号等等。



### 2，逻辑回归LR

逻辑回归是用来解决二分类问题的机器学习方法，用于估计某种事物的可能性。

逻辑回归中x ——> hypothesis（假设）——> y，sigmoid函数将预测值转换为0-1之间的值。

$$0 \leq h_{\theta}(x) \leqslant 1$$

$$h_{\theta}(x)=g\left(\theta^{\top} x\right)$$

$$g(z)=\frac{1}{1+e^{-z}}$$

解释：$h_{\theta}(x)$ 是对输入x，y=1的概率估计，$h_{\theta}(x)=P(y=1 / x ; \theta)$ 给定特征x与参数$\theta$时，y=1的概率。也可以这样理解，一个事件的发生几率指的是该事件发生的概率p与该事件不发生的概率1-p的比值。

$$logit(p) = \log{\frac{p}{1-p}}$$

$$\log \frac{P(Y=1|x)}{1-P(Y=1|x)} = \theta^T x$$

LR中，输出Y=1的对数几率是输入x的线性函数。



损失函数这里有所不同，因为sigmoid函数代入到平方误差中得到$J$ 是非凸函数，所以cost function用的是交叉熵（信息量中度量不确定性的度量）。

$$J\left(h_{\theta}(x), y\right)=-\frac{1}{m} [ \sum_{i=1}^m y^{(i)} \log \left(h_{\theta}(x^{(i)})\right) + (1-y^{(i)}) \log \left(1-h_{\theta}(x^{(i)})\right) ]$$

这个非常像似然函数。



LR的梯度下降公式（对各个参数的偏导 or 链式求导），因为sigmoid函数求导特殊g(z)' = g(z) (1-g(z))'。

$$\theta_j = \theta_j - \alpha \frac{\partial}{\partial \theta_j} J(\theta)$$

$$\theta_{j}=\theta_j-\alpha\frac{1}{m} \sum_{i=1}^{m}\left(h_{\theta}\left(x^{(i)}\right)-y^{(i)}\right) x_{j}^{(i)}$$

推导过程：

$$\frac{\partial}{\partial \theta_j} J(\theta) =-\frac{1}{m} \sum_{i=1}^{m}\left[y^{(i)} \frac{1}{\left.h_{\theta}\left(x^{(i)}\right)\right)} \frac{\partial}{\partial \theta_{j}} h_{\theta}\left(x^{(i)}\right)-\left(1-y^{(i)}\right) \frac{1}{1-h_{\theta}\left(x^{(i)}\right)} \frac{\partial}{\partial \theta_{j}} h_{\theta}\left(x^{(i)}\right)\right]$$

$$\frac{\partial}{\partial \theta_j} h_\theta(x^{(i)}) = \frac{e^{-\theta^{T} x^{(i)}}}{\left(1+e^{-\theta^{T} x^{(i)}}\right)^{2}} \frac{\partial}{\partial \theta_{j}} \theta^{T} x^{(i)} = g\left(\theta^{T} x^{(i)}\right)\left(1-g\left(\theta^{T} x^{(i)}\right)\right) x_{j}^{(i)}$$

最后代入化简即可。

其实逻辑回归还可以用于多分类问题上，分别拟合三个分类器$h_{\theta}^{(i)}(x)$，选择 $max {h_{\theta}^{(i)}(x)}$的类i。

**注意：**

优点：

1，对逻辑回归来说，多重共线性并不是问题，它可以结合L2正则化来解决

2，属于判别式模型

3，在线梯度下降算法-online gradient descent

4，便利的观测样本概率分数

缺点

1，特征空间很大时，逻辑回归的性能不是很好（不能很好地处理大量多类特征或变量，one hot很大）

2，容易欠拟合，一般准确度不太高；

3，对于非线性特征，需要进行转换。



### 3，过拟合与正则化

#### 3.1 欠拟合与过拟合

欠拟合：模型过于简单，underfit，带来高偏差high bias，就是说模型偏见很强；

过拟合：模型复杂，overfit，太过于拟合训练数据，经验误差虽小但结构误差大，无法拟合新数据。

![20191209overfit](/images/20191209overfit.jpg)

避免过拟合的方法：数据增强more data，简化模型（早停，限制权值正则化，多种模型Bagging，Boosting），增加噪声，集成ensemble，贝叶斯等。

#### 3.2 正则化

正则化，损失部分尽量拟合数据，后面部分尽量保持参数较小，起到正则化作用。

线性回归的损失函数：

$$J(\theta) = \frac{1}{2m}[\sum_{i=1}^{m}(h_\theta(x^{(i)}) - y^{(i)} )^2 + \lambda \sum_{i=1}^{n} \theta_j^2]$$

逻辑回归的损失函数

$$J(\theta)=-\left[\frac{1}{m} \sum_{i=1}^{m} y^{(i)} \log \left(h_{\theta}(x^{(i)}\right)+\left(1-y^{(i)}\right) \log \left(1-h_{\theta}\left(x^{(i)}\right)\right)\right] + \frac{\lambda}{2m} \sum_{j=1}^n\theta_j^2$$



### 4，优化方法复习

#### 4.1 最速下降法即负梯度方向

$$f(x_k+\alpha \vec P) = f(x_k) + \alpha P^T \nabla f(x_k) + o(\alpha)$$

$$min{f(x_k+\alpha \vec P)}$$

则需要$min{P^T \nabla f(x_k)}$

$$P = -\frac{\nabla f(x_k)}{||\nabla f(x_k)||}$$

此处$\alpha$是学习率

一般来说，损失函数偏碗状的时候，比较圆的时候，下降比较快。如果，函数形状椭圆形，会来回震荡着走。



#### 4.2 牛顿法

牛顿法考虑二阶导信息

$$f\left(x_{k} + P\right)=f\left(x_{k}\right)+p^T \nabla f_{k}+\frac{1}{2} p^{T} \nabla^{2} f_{k} P$$

则$P = - \nabla^2 f(x_k)^{-1} \nabla f(x_k)$

此处二阶导大于0，即要求$\nabla^2 f(x_k)$ 正定。



总结：逻辑回归主要是增加了一个sigmoid函数，将预测值映射为概率。为了避免损失函数变为非凸函数，损失函数变为对数损失函数。

