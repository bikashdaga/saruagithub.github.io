---
title: DNN概述
date: 2019-12-24 10:18:37
mathjax: true
tags:
- 深度学习
- 神经网络
categories:
- 深度学习
---



### 概述

从最开始的神经元开始，1943年MP单个神经元诞生。$y = f\left(\sum_{i=1}^{n} w_{i} x_{i}-\theta\right)$

![20200223MP_nerual](/images/20200223MP_nerual.jpg)

1958年，感知机perception提出。感知机能很好的实现逻辑与、或、非运算。与、或、非本来也是线性可分问题。若两类模式是线性可分的，即存在一个线性超平面将它们分开。

若要解决非线性可分问题，要使用多层功能神经元（1982-1986年，1995提出SVM）。两层感知机可以解决异或问题了。一个神经网络足够多的两层网络 + 适当的激活函数能够逼近任意连续的函数。

2012年，DNN，CNN由于计算能力的提高再次兴起。

![20200223DNN](/images/20200223DNN.jpg)



### 反向传播

正向传播就是将输入与权重相乘，逐层向前，直到最后输出。然后计算输出层的误差，反向传播则是将误差逆向传播至隐层神经元。根据隐层神经元的误差对权值和阈值进行更新。

![20200223DNN_BP](/images/20200223DNN_BP.jpg)

$z$ 是样本真实值，$y = f (e), e= wx + b$ 是DNN预测值，激活函数 $f$ 采用 sigmoid，采用均方误差 $\frac{1}{2}(y-z)^2$。

机器学习书上推导是根据链式求导：

$$\delta_{6}=(y-z) \frac{d f_6(e)}{d e}=(y-z) f_6(e)(1-f_6(e))$$

$$\delta_{4}=\mathrm{w}_{46} \delta_{6} \frac{\mathrm{df}_{4}(\mathrm{e})}{d e}, \delta_{5}=\mathrm{W}_{56} \mathrm{\delta}_{6} \frac{\mathrm{df_4}(\mathrm{e})}{d e}$$

$$\delta_{1}=\left(\mathrm{w}_{14} \mathrm{\delta}_{4}+\mathrm{w}_{15} \mathrm{\delta}_{5}\right) \frac{\mathrm{df}_{1}(\mathrm{e})}{d e}, \quad \delta_{2}=\left(\mathrm{w}_{24} \delta_{4}+\mathrm{w}_{25} \delta_{5}\right) \frac{\mathrm{d} f_{2}(\mathrm{e})}{d e}, \delta_{3}=\left(\mathrm{W}_{34} \mathrm{\delta}_{4}+\mathrm{W}_{35} \mathrm{\delta}_{5}\right) \frac{\mathrm{d} \mathrm{f}_3(\mathrm{e})}{d e}$$

同理......



### 一些问题

1，目前网络层次越深能够抽象出来的特征越多，但随着神经网络层数的加深，计算量越大，优化函数越来越容易陷入局部最优解。

2，随着网络层数增加，“梯度消失”现象更加严重。



### Reference

1，智能技术基础，罗晓鹏

2，机器学习，周志华